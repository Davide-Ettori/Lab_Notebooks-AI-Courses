{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "BFGS.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyObgaPNyGizPxRKSWVCGz56"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdCkH2JM07eu"
   },
   "source": [
    "# Quasi-Newton methods: BFGS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T9vAduWLjiTl",
    "ExecuteTime": {
     "end_time": "2023-12-18T14:06:01.725581Z",
     "start_time": "2023-12-18T14:06:00.183898Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import scipy.optimize\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We enable double precision in JAX\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQmGG5duO6ig"
   },
   "source": [
    "Consider the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function), that is minimized in $\\mathbf{x} = (1,1,\\dots,1)^T$:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{x}) = \\sum_{i=1}^{N-1} [100 (x_{i+1} - x_i^2 )^2 + (1-x_i)^2]$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ybmumhr1z32F",
    "ExecuteTime": {
     "end_time": "2023-12-18T14:06:01.730482Z",
     "start_time": "2023-12-18T14:06:01.728263Z"
    }
   },
   "source": [
    "def loss(x):\n",
    "    return sum(100.0 * (x[1:] - x[:-1] ** 2) ** 2 + (1 - x[:-1]) ** 2)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opnXVnczPDLj"
   },
   "source": [
    "Use `jax` to compute and compile the Rosenbrock function and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(jax.grad(loss, argnums=0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T14:06:01.736223Z",
     "start_time": "2023-12-18T14:06:01.732927Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEvFRykGQtzx"
   },
   "source": [
    "Implement the BFGS method (with line search) for the minimization of the Rosenbrock function.\n",
    "Set a maximum of 1000 epochs and a stopping tolerance on the gradient eucledian norm of $10^{-8}$. Employ an initial guess for $\\mathbf{x}$ with random numbers in the interval $[0,2]$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t3IJUnuTZdS1",
    "ExecuteTime": {
     "end_time": "2023-12-18T14:06:15.486017Z",
     "start_time": "2023-12-18T14:06:14.868421Z"
    }
   },
   "source": [
    "N = 100\n",
    "max_epochs = 1000\n",
    "tol = 1e-8\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.uniform(0, 2, N)\n",
    "dx = grad_jit(x)\n",
    "I = jnp.eye(N)\n",
    "B_inv = I.copy()\n",
    "history = list([loss_jit(x)])\n",
    "\n",
    "epoch = 0\n",
    "while epoch < max_epochs and np.linalg.norm(dx) > tol:\n",
    "    epoch += 1\n",
    "    # search direction\n",
    "    dir = -(B_inv @ dx)\n",
    "    # line search\n",
    "    lr = sp.optimize.line_search(loss_jit, grad_jit, x, dir, maxiter=1000)[0] # lr respect automatically wolf condition, thanks scipy\n",
    "    x_new = x + lr * dir # use + sign because you inverted the sign of dir (before)\n",
    "    dx_new = grad_jit(x_new)\n",
    "    # Sherman-Morrison update\n",
    "    s = x_new - x\n",
    "    y = dx_new - dx\n",
    "    E = I - np.outer(y, s) / np.inner(y, s)\n",
    "    B_inv = E.T @ B_inv @ E + np.outer(s, s) / np.inner(y, s)\n",
    "    l = loss_jit(x_new)\n",
    "    history.append(l)\n",
    "    x = x_new\n",
    "    dx = dx_new\n",
    "    # print updates\n",
    "    print(f\"epoch {epoch}\")\n",
    "    print(f\"loss: {l}\")\n",
    "    print(f\"gradient norm: {np.linalg.norm(dx)}\")\n",
    "\n",
    "print(f\"\\n\\nNorm of the difference between real solution and found solution: {np.linalg.norm(x - np.ones(x.shape))}\")"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "loss: 6280.107001306367\n",
      "gradient norm: 4530.527872576332\n",
      "epoch 2\n",
      "loss: 3546.6756039477614\n",
      "gradient norm: 2669.8769399107296\n",
      "epoch 3\n",
      "loss: 3139.207524653895\n",
      "gradient norm: 2314.7958658662646\n",
      "epoch 4\n",
      "loss: 2539.008813292769\n",
      "gradient norm: 1821.56594629532\n",
      "epoch 5\n",
      "loss: 2202.563142547062\n",
      "gradient norm: 1471.8908975532315\n",
      "epoch 6\n",
      "loss: 2010.1138228553934\n",
      "gradient norm: 1367.0924362488195\n",
      "epoch 7\n",
      "loss: 1922.158432616825\n",
      "gradient norm: 1258.1102578730665\n",
      "epoch 8\n",
      "loss: 1872.907222708664\n",
      "gradient norm: 1252.859661396601\n",
      "epoch 9\n",
      "loss: 1833.9544431531815\n",
      "gradient norm: 1201.1761340820083\n",
      "epoch 10\n",
      "loss: 1821.7746362871774\n",
      "gradient norm: 1175.8313284017452\n",
      "epoch 11\n",
      "loss: 1801.7180391071145\n",
      "gradient norm: 1153.8701641850214\n",
      "epoch 12\n",
      "loss: 1759.0696873448665\n",
      "gradient norm: 1094.1213272292946\n",
      "epoch 13\n",
      "loss: 1720.2874311914754\n",
      "gradient norm: 1058.6321362372305\n",
      "epoch 14\n",
      "loss: 1679.5276586090877\n",
      "gradient norm: 1092.0510684027167\n",
      "epoch 15\n",
      "loss: 1625.7924188156742\n",
      "gradient norm: 1048.6642791424238\n",
      "epoch 16\n",
      "loss: 1565.8352363505267\n",
      "gradient norm: 1170.6042594502785\n",
      "epoch 17\n",
      "loss: 1508.8819754540027\n",
      "gradient norm: 1183.1644941216782\n",
      "epoch 18\n",
      "loss: 1439.1891711156309\n",
      "gradient norm: 1036.4338987506076\n",
      "epoch 19\n",
      "loss: 1356.0209996191718\n",
      "gradient norm: 991.3656082889862\n",
      "epoch 20\n",
      "loss: 1225.218882542143\n",
      "gradient norm: 831.5674751834683\n",
      "epoch 21\n",
      "loss: 946.6538925743304\n",
      "gradient norm: 800.9897837460157\n",
      "epoch 22\n",
      "loss: 830.6762335462386\n",
      "gradient norm: 583.3591676071838\n",
      "epoch 23\n",
      "loss: 807.7286281853596\n",
      "gradient norm: 571.5976711845402\n",
      "epoch 24\n",
      "loss: 784.5861988364979\n",
      "gradient norm: 570.8895698540016\n",
      "epoch 25\n",
      "loss: 769.2623143295434\n",
      "gradient norm: 560.6791786922533\n",
      "epoch 26\n",
      "loss: 739.7717809163087\n",
      "gradient norm: 547.2281958304975\n",
      "epoch 27\n",
      "loss: 672.2004262113209\n",
      "gradient norm: 506.6975453967628\n",
      "epoch 28\n",
      "loss: 557.0743421249566\n",
      "gradient norm: 407.76781885818116\n",
      "epoch 29\n",
      "loss: 514.7051169805412\n",
      "gradient norm: 405.5145855808427\n",
      "epoch 30\n",
      "loss: 486.85513565351647\n",
      "gradient norm: 365.1540673327682\n",
      "epoch 31\n",
      "loss: 475.95545152486625\n",
      "gradient norm: 353.74001551259397\n",
      "epoch 32\n",
      "loss: 469.7470574101799\n",
      "gradient norm: 351.21708272162994\n",
      "epoch 33\n",
      "loss: 466.0588930085831\n",
      "gradient norm: 349.752040087701\n",
      "epoch 34\n",
      "loss: 462.2511541519901\n",
      "gradient norm: 346.31299099579167\n",
      "epoch 35\n",
      "loss: 458.3939364172763\n",
      "gradient norm: 345.9823909529476\n",
      "epoch 36\n",
      "loss: 453.3431722378698\n",
      "gradient norm: 341.5192899794744\n",
      "epoch 37\n",
      "loss: 447.75711671761724\n",
      "gradient norm: 337.1021237511282\n",
      "epoch 38\n",
      "loss: 446.0518342138287\n",
      "gradient norm: 333.749474442226\n",
      "epoch 39\n",
      "loss: 439.83095447447124\n",
      "gradient norm: 332.36756579032243\n",
      "epoch 40\n",
      "loss: 425.39467726709944\n",
      "gradient norm: 324.02138263447983\n",
      "epoch 41\n",
      "loss: 367.2562124223289\n",
      "gradient norm: 288.33195128387837\n",
      "epoch 42\n",
      "loss: 317.15255859908575\n",
      "gradient norm: 259.6656654208563\n",
      "epoch 43\n",
      "loss: 294.13506329687146\n",
      "gradient norm: 260.34225490520515\n",
      "epoch 44\n",
      "loss: 279.50523752039544\n",
      "gradient norm: 254.58035569234983\n",
      "epoch 45\n",
      "loss: 272.6744318518394\n",
      "gradient norm: 254.44519768126395\n",
      "epoch 46\n",
      "loss: 270.4340665402783\n",
      "gradient norm: 251.919522741153\n",
      "epoch 47\n",
      "loss: 267.8688785056464\n",
      "gradient norm: 248.0462924383085\n",
      "epoch 48\n",
      "loss: 257.6279983899919\n",
      "gradient norm: 237.74322367757753\n",
      "epoch 49\n",
      "loss: 232.54775917094642\n",
      "gradient norm: 217.3667679245889\n",
      "epoch 50\n",
      "loss: 206.73939742799274\n",
      "gradient norm: 197.36188442850082\n",
      "epoch 51\n",
      "loss: 198.7085495383119\n",
      "gradient norm: 200.2121568227175\n",
      "epoch 52\n",
      "loss: 195.88887596150036\n",
      "gradient norm: 207.16118835189334\n",
      "epoch 53\n",
      "loss: 192.35967979109358\n",
      "gradient norm: 202.61187222881927\n",
      "epoch 54\n",
      "loss: 182.2160143450516\n",
      "gradient norm: 192.98025085272164\n",
      "epoch 55\n",
      "loss: 170.57751286296113\n",
      "gradient norm: 172.7761274187408\n",
      "epoch 56\n",
      "loss: 168.77728749355242\n",
      "gradient norm: 171.08889154703698\n",
      "epoch 57\n",
      "loss: 167.39023380207158\n",
      "gradient norm: 167.8154112337246\n",
      "epoch 58\n",
      "loss: 156.77447715780468\n",
      "gradient norm: 157.0342006525322\n",
      "epoch 59\n",
      "loss: 146.4575220024923\n",
      "gradient norm: 140.28921951883638\n",
      "epoch 60\n",
      "loss: 143.55658072427363\n",
      "gradient norm: 132.00462829289958\n",
      "epoch 61\n",
      "loss: 143.13632296896702\n",
      "gradient norm: 130.9521213489708\n",
      "epoch 62\n",
      "loss: 141.09824319324463\n",
      "gradient norm: 128.73987906320954\n",
      "epoch 63\n",
      "loss: 129.8468126993566\n",
      "gradient norm: 111.34150743482012\n",
      "epoch 64\n",
      "loss: 124.25322558412005\n",
      "gradient norm: 105.27365867060954\n",
      "epoch 65\n",
      "loss: 123.9396537207741\n",
      "gradient norm: 104.82714124876856\n",
      "epoch 66\n",
      "loss: 122.96257935306618\n",
      "gradient norm: 103.41352444675505\n",
      "epoch 67\n",
      "loss: 116.08291479087002\n",
      "gradient norm: 89.27350736999794\n",
      "epoch 68\n",
      "loss: 115.06714594501577\n",
      "gradient norm: 86.46498493415163\n",
      "epoch 69\n",
      "loss: 114.94129962785784\n",
      "gradient norm: 85.99922760811316\n",
      "epoch 70\n",
      "loss: 109.47806172834919\n",
      "gradient norm: 67.77164501027993\n",
      "epoch 71\n",
      "loss: 105.29023701768752\n",
      "gradient norm: 51.04357862052686\n",
      "epoch 72\n",
      "loss: 105.1971587765368\n",
      "gradient norm: 50.47120947537411\n",
      "epoch 73\n",
      "loss: 104.2329246038723\n",
      "gradient norm: 46.00561275827098\n",
      "epoch 74\n",
      "loss: 101.42794092678741\n",
      "gradient norm: 33.00071304197513\n",
      "epoch 75\n",
      "loss: 101.37103584730119\n",
      "gradient norm: 32.71921869432647\n",
      "epoch 76\n",
      "loss: 100.93107567133274\n",
      "gradient norm: 30.06342697102043\n",
      "epoch 77\n",
      "loss: 99.7928974207135\n",
      "gradient norm: 21.5086380075891\n",
      "epoch 78\n",
      "loss: 99.78404938542394\n",
      "gradient norm: 21.431115859725267\n",
      "epoch 79\n",
      "loss: 99.3559478774304\n",
      "gradient norm: 17.624014324397866\n",
      "epoch 80\n",
      "loss: 99.14117858624147\n",
      "gradient norm: 15.219563053706635\n",
      "epoch 81\n",
      "loss: 99.1334117612102\n",
      "gradient norm: 15.133301049747125\n",
      "epoch 82\n",
      "loss: 98.88486056906868\n",
      "gradient norm: 11.314911009764229\n",
      "epoch 83\n",
      "loss: 98.87775908452491\n",
      "gradient norm: 11.175433110300972\n",
      "epoch 84\n",
      "loss: 98.86421042374255\n",
      "gradient norm: 11.10299444433426\n",
      "epoch 85\n",
      "loss: 98.73166483176917\n",
      "gradient norm: 8.871457960165062\n",
      "epoch 86\n",
      "loss: 98.72898682400724\n",
      "gradient norm: 8.830071366300938\n",
      "epoch 87\n",
      "loss: 98.59519306846259\n",
      "gradient norm: 5.796850947173661\n",
      "epoch 88\n",
      "loss: 98.59057307696015\n",
      "gradient norm: 5.665186650027239\n",
      "epoch 89\n",
      "loss: 98.57415024722205\n",
      "gradient norm: 5.382801103957592\n",
      "epoch 90\n",
      "loss: 98.52447040358983\n",
      "gradient norm: 4.43425296775064\n",
      "epoch 91\n",
      "loss: 98.52388590320783\n",
      "gradient norm: 4.425403436958036\n",
      "epoch 92\n",
      "loss: 98.48721463167284\n",
      "gradient norm: 4.013476022586509\n",
      "epoch 93\n",
      "loss: 98.48563833988028\n",
      "gradient norm: 4.0051772803858\n",
      "epoch 94\n",
      "loss: 98.46209146631149\n",
      "gradient norm: 4.425056548921127\n",
      "epoch 95\n",
      "loss: 98.40159412342494\n",
      "gradient norm: 6.172592256950631\n",
      "epoch 96\n",
      "loss: 98.40058198157541\n",
      "gradient norm: 6.197313519097772\n",
      "epoch 97\n",
      "loss: 98.30594217186756\n",
      "gradient norm: 7.215850621951867\n",
      "epoch 98\n",
      "loss: 98.30117046882383\n",
      "gradient norm: 7.275999340359665\n",
      "epoch 99\n",
      "loss: 98.27423591140365\n",
      "gradient norm: 6.708973121329337\n",
      "epoch 100\n",
      "loss: 98.2515120210964\n",
      "gradient norm: 6.138829997517735\n",
      "epoch 101\n",
      "loss: 98.23432938916933\n",
      "gradient norm: 6.616499768661252\n",
      "epoch 102\n",
      "loss: 98.18928520043416\n",
      "gradient norm: 8.181771466619848\n",
      "epoch 103\n",
      "loss: 98.18720822772909\n",
      "gradient norm: 8.109329133503964\n",
      "epoch 104\n",
      "loss: 98.1338989622208\n",
      "gradient norm: 5.081830097334769\n",
      "epoch 105\n",
      "loss: 98.13306844331845\n",
      "gradient norm: 5.04931148493595\n",
      "epoch 106\n",
      "loss: 98.06229735876425\n",
      "gradient norm: 5.7145794503556\n",
      "epoch 107\n",
      "loss: 98.02548993366456\n",
      "gradient norm: 6.881571161298668\n",
      "epoch 108\n",
      "loss: 98.01856760977465\n",
      "gradient norm: 7.0414694923150165\n",
      "epoch 109\n",
      "loss: 97.93842852394357\n",
      "gradient norm: 9.026505942942038\n",
      "epoch 110\n",
      "loss: 97.93694575384218\n",
      "gradient norm: 9.011739431451423\n",
      "epoch 111\n",
      "loss: 97.8753797210323\n",
      "gradient norm: 7.007008078532464\n",
      "epoch 112\n",
      "loss: 97.87274462101047\n",
      "gradient norm: 6.932890925943849\n",
      "epoch 113\n",
      "loss: 97.77469345700683\n",
      "gradient norm: 7.158773943765023\n",
      "epoch 114\n",
      "loss: 97.69828648499903\n",
      "gradient norm: 8.105016106881445\n",
      "epoch 115\n",
      "loss: 97.69433974082743\n",
      "gradient norm: 8.1438255776454\n",
      "epoch 116\n",
      "loss: 97.58292794398382\n",
      "gradient norm: 10.600277677767545\n",
      "epoch 117\n",
      "loss: 97.5810202297227\n",
      "gradient norm: 10.579505332264219\n",
      "epoch 118\n",
      "loss: 97.50730220830059\n",
      "gradient norm: 8.225229003907488\n",
      "epoch 119\n",
      "loss: 97.50506807572185\n",
      "gradient norm: 8.184490165274575\n",
      "epoch 120\n",
      "loss: 97.31388512453131\n",
      "gradient norm: 8.530418402754997\n",
      "epoch 121\n",
      "loss: 97.27399293061747\n",
      "gradient norm: 9.183945678711028\n",
      "epoch 122\n",
      "loss: 97.2076543160585\n",
      "gradient norm: 10.617204935542611\n",
      "epoch 123\n",
      "loss: 97.13155678549215\n",
      "gradient norm: 14.014038941018306\n",
      "epoch 124\n",
      "loss: 97.06438596797467\n",
      "gradient norm: 12.859982337778732\n",
      "epoch 125\n",
      "loss: 96.9886942591856\n",
      "gradient norm: 16.225367778805463\n",
      "epoch 126\n",
      "loss: 96.97682113523034\n",
      "gradient norm: 16.047178836238473\n",
      "epoch 127\n",
      "loss: 96.61370220388164\n",
      "gradient norm: 11.157319817405863\n",
      "epoch 128\n",
      "loss: 96.60698156927442\n",
      "gradient norm: 11.049083809528558\n",
      "epoch 129\n",
      "loss: 96.3861354423006\n",
      "gradient norm: 7.736442086034194\n",
      "epoch 130\n",
      "loss: 96.38134249358473\n",
      "gradient norm: 7.766281997952927\n",
      "epoch 131\n",
      "loss: 96.2575009205797\n",
      "gradient norm: 8.985888688811185\n",
      "epoch 132\n",
      "loss: 96.24986455730854\n",
      "gradient norm: 9.167309334371785\n",
      "epoch 133\n",
      "loss: 96.07397752952029\n",
      "gradient norm: 10.315283151739692\n",
      "epoch 134\n",
      "loss: 96.06845721142201\n",
      "gradient norm: 10.446864844759743\n",
      "epoch 135\n",
      "loss: 95.84265799004493\n",
      "gradient norm: 11.323641486459985\n",
      "epoch 136\n",
      "loss: 95.83632929642401\n",
      "gradient norm: 11.36258581016553\n",
      "epoch 137\n",
      "loss: 95.5991157759097\n",
      "gradient norm: 8.764554886350703\n",
      "epoch 138\n",
      "loss: 95.43775733657462\n",
      "gradient norm: 9.705869708039593\n",
      "epoch 139\n",
      "loss: 95.10578056482919\n",
      "gradient norm: 10.019050256409448\n",
      "epoch 140\n",
      "loss: 94.83229752041376\n",
      "gradient norm: 9.98310447230147\n",
      "epoch 141\n",
      "loss: 94.67897407979183\n",
      "gradient norm: 10.540455587496693\n",
      "epoch 142\n",
      "loss: 94.2950799937559\n",
      "gradient norm: 17.04398415718038\n",
      "epoch 143\n",
      "loss: 94.08640439535606\n",
      "gradient norm: 10.996795033053981\n",
      "epoch 144\n",
      "loss: 93.76774126228528\n",
      "gradient norm: 14.484276208389405\n",
      "epoch 145\n",
      "loss: 93.51048855809286\n",
      "gradient norm: 12.523540845670588\n",
      "epoch 146\n",
      "loss: 93.25464095460572\n",
      "gradient norm: 10.991650841796039\n",
      "epoch 147\n",
      "loss: 93.10654904305689\n",
      "gradient norm: 19.06961137200946\n",
      "epoch 148\n",
      "loss: 92.72544391604313\n",
      "gradient norm: 8.220453901282543\n",
      "epoch 149\n",
      "loss: 92.45045931489527\n",
      "gradient norm: 10.181858311375683\n",
      "epoch 150\n",
      "loss: 92.18428244141604\n",
      "gradient norm: 14.881547353365136\n",
      "epoch 151\n",
      "loss: 92.00314748848187\n",
      "gradient norm: 12.745097295301463\n",
      "epoch 152\n",
      "loss: 91.75284611944305\n",
      "gradient norm: 10.39231565214986\n",
      "epoch 153\n",
      "loss: 91.47354469824289\n",
      "gradient norm: 9.63849912329557\n",
      "epoch 154\n",
      "loss: 91.2891378142593\n",
      "gradient norm: 9.692336957367868\n",
      "epoch 155\n",
      "loss: 91.0198674813341\n",
      "gradient norm: 13.376856599288422\n",
      "epoch 156\n",
      "loss: 90.82701560388284\n",
      "gradient norm: 10.556928477813333\n",
      "epoch 157\n",
      "loss: 90.5037227865104\n",
      "gradient norm: 13.931123746917073\n",
      "epoch 158\n",
      "loss: 90.15701162365843\n",
      "gradient norm: 12.311511863530809\n",
      "epoch 159\n",
      "loss: 89.9762268308918\n",
      "gradient norm: 17.333255044976873\n",
      "epoch 160\n",
      "loss: 89.87169797826861\n",
      "gradient norm: 17.55127646692472\n",
      "epoch 161\n",
      "loss: 89.45062906100522\n",
      "gradient norm: 13.577992731613323\n",
      "epoch 162\n",
      "loss: 89.22337379551158\n",
      "gradient norm: 12.051887795735187\n",
      "epoch 163\n",
      "loss: 88.9459059998477\n",
      "gradient norm: 9.88044976096507\n",
      "epoch 164\n",
      "loss: 88.71871570630961\n",
      "gradient norm: 17.96377376088138\n",
      "epoch 165\n",
      "loss: 88.55858491190989\n",
      "gradient norm: 14.906147486735783\n",
      "epoch 166\n",
      "loss: 88.40661077314314\n",
      "gradient norm: 18.87710028044678\n",
      "epoch 167\n",
      "loss: 88.05931399899424\n",
      "gradient norm: 8.791734658238099\n",
      "epoch 168\n",
      "loss: 87.83158053859447\n",
      "gradient norm: 15.392959503285212\n",
      "epoch 169\n",
      "loss: 87.58871348771059\n",
      "gradient norm: 16.113957641980953\n",
      "epoch 170\n",
      "loss: 87.27078241354765\n",
      "gradient norm: 13.145111642464693\n",
      "epoch 171\n",
      "loss: 87.08392062950227\n",
      "gradient norm: 11.726755462679378\n",
      "epoch 172\n",
      "loss: 86.82051075392467\n",
      "gradient norm: 11.71911498929974\n",
      "epoch 173\n",
      "loss: 86.61067203000115\n",
      "gradient norm: 10.856713540474434\n",
      "epoch 174\n",
      "loss: 86.35856936499728\n",
      "gradient norm: 10.432494833805672\n",
      "epoch 175\n",
      "loss: 86.1375440114608\n",
      "gradient norm: 10.994859642931925\n",
      "epoch 176\n",
      "loss: 86.0256248308938\n",
      "gradient norm: 9.70424209636475\n",
      "epoch 177\n",
      "loss: 85.88634861782634\n",
      "gradient norm: 11.546375949848983\n",
      "epoch 178\n",
      "loss: 85.73433203575662\n",
      "gradient norm: 10.961757934857948\n",
      "epoch 179\n",
      "loss: 85.52832901984718\n",
      "gradient norm: 10.05973998921372\n",
      "epoch 180\n",
      "loss: 85.26134638098353\n",
      "gradient norm: 11.308764349577045\n",
      "epoch 181\n",
      "loss: 85.05161250924249\n",
      "gradient norm: 13.106259719529874\n",
      "epoch 182\n",
      "loss: 84.86681769554271\n",
      "gradient norm: 12.757599497084366\n",
      "epoch 183\n",
      "loss: 84.56240302795669\n",
      "gradient norm: 12.781372909724666\n",
      "epoch 184\n",
      "loss: 84.26209836856998\n",
      "gradient norm: 11.10164260937415\n",
      "epoch 185\n",
      "loss: 83.9490779293989\n",
      "gradient norm: 12.828147210712135\n",
      "epoch 186\n",
      "loss: 83.83077394159066\n",
      "gradient norm: 12.895070397401064\n",
      "epoch 187\n",
      "loss: 83.44269221880366\n",
      "gradient norm: 9.529639390422215\n",
      "epoch 188\n",
      "loss: 83.17168608399052\n",
      "gradient norm: 9.128699152244376\n",
      "epoch 189\n",
      "loss: 82.88554408591648\n",
      "gradient norm: 10.552130581486777\n",
      "epoch 190\n",
      "loss: 82.66389979761044\n",
      "gradient norm: 12.209707262929124\n",
      "epoch 191\n",
      "loss: 82.2953571627806\n",
      "gradient norm: 14.553357002607745\n",
      "epoch 192\n",
      "loss: 82.00867214832967\n",
      "gradient norm: 10.97257448687738\n",
      "epoch 193\n",
      "loss: 81.7337593211968\n",
      "gradient norm: 12.08132079259548\n",
      "epoch 194\n",
      "loss: 81.53666067426371\n",
      "gradient norm: 12.241467703379083\n",
      "epoch 195\n",
      "loss: 81.33975616275563\n",
      "gradient norm: 14.640552244899583\n",
      "epoch 196\n",
      "loss: 81.04965577436661\n",
      "gradient norm: 12.520642858124782\n",
      "epoch 197\n",
      "loss: 80.80947752445772\n",
      "gradient norm: 13.246559089345247\n",
      "epoch 198\n",
      "loss: 80.48735377815517\n",
      "gradient norm: 14.389708643185337\n",
      "epoch 199\n",
      "loss: 80.27774527514262\n",
      "gradient norm: 11.610853853355412\n",
      "epoch 200\n",
      "loss: 80.001359126072\n",
      "gradient norm: 13.14754204684669\n",
      "epoch 201\n",
      "loss: 79.73634611537413\n",
      "gradient norm: 15.924491399223713\n",
      "epoch 202\n",
      "loss: 79.47701044087658\n",
      "gradient norm: 13.890932228447271\n",
      "epoch 203\n",
      "loss: 79.31446390063856\n",
      "gradient norm: 22.72643364521666\n",
      "epoch 204\n",
      "loss: 78.97735216660153\n",
      "gradient norm: 10.482116884672655\n",
      "epoch 205\n",
      "loss: 78.7855846687584\n",
      "gradient norm: 16.41537043930589\n",
      "epoch 206\n",
      "loss: 78.40647367753088\n",
      "gradient norm: 15.28230175626204\n",
      "epoch 207\n",
      "loss: 78.15789551746917\n",
      "gradient norm: 11.897567872270931\n",
      "epoch 208\n",
      "loss: 77.838727615316\n",
      "gradient norm: 15.559239020862575\n",
      "epoch 209\n",
      "loss: 77.5884480241904\n",
      "gradient norm: 19.12205428590487\n",
      "epoch 210\n",
      "loss: 77.15923034217299\n",
      "gradient norm: 9.616893818386977\n",
      "epoch 211\n",
      "loss: 77.02779479502381\n",
      "gradient norm: 10.770414571487622\n",
      "epoch 212\n",
      "loss: 76.75949211161276\n",
      "gradient norm: 21.32830099151727\n",
      "epoch 213\n",
      "loss: 76.38868258970173\n",
      "gradient norm: 15.98004820325147\n",
      "epoch 214\n",
      "loss: 76.18678586769737\n",
      "gradient norm: 15.570725733656765\n",
      "epoch 215\n",
      "loss: 76.04703529722454\n",
      "gradient norm: 24.75104379953234\n",
      "epoch 216\n",
      "loss: 75.56238733640521\n",
      "gradient norm: 12.936761790129504\n",
      "epoch 217\n",
      "loss: 75.308833531368\n",
      "gradient norm: 8.78495057328362\n",
      "epoch 218\n",
      "loss: 75.01209718184464\n",
      "gradient norm: 13.977308753253988\n",
      "epoch 219\n",
      "loss: 74.79055233099987\n",
      "gradient norm: 13.015052343717084\n",
      "epoch 220\n",
      "loss: 74.57255436453377\n",
      "gradient norm: 13.597897489154937\n",
      "epoch 221\n",
      "loss: 74.29801201173845\n",
      "gradient norm: 13.120284285222214\n",
      "epoch 222\n",
      "loss: 74.08663898929242\n",
      "gradient norm: 17.546730505309267\n",
      "epoch 223\n",
      "loss: 73.57394790747428\n",
      "gradient norm: 12.532491010826368\n",
      "epoch 224\n",
      "loss: 73.42483121290158\n",
      "gradient norm: 11.657032945687709\n",
      "epoch 225\n",
      "loss: 73.07706585948888\n",
      "gradient norm: 15.27206263500134\n",
      "epoch 226\n",
      "loss: 72.77459739209662\n",
      "gradient norm: 14.343178664300176\n",
      "epoch 227\n",
      "loss: 72.46425617623146\n",
      "gradient norm: 10.760293326557608\n",
      "epoch 228\n",
      "loss: 72.36531125204124\n",
      "gradient norm: 10.920182786625888\n",
      "epoch 229\n",
      "loss: 72.10210458804141\n",
      "gradient norm: 15.7475507048396\n",
      "epoch 230\n",
      "loss: 71.88713678795094\n",
      "gradient norm: 13.872408906127903\n",
      "epoch 231\n",
      "loss: 71.64548527164544\n",
      "gradient norm: 10.878118363171147\n",
      "epoch 232\n",
      "loss: 71.46100534505759\n",
      "gradient norm: 10.956057104662936\n",
      "epoch 233\n",
      "loss: 71.23448035531807\n",
      "gradient norm: 11.52197511499328\n",
      "epoch 234\n",
      "loss: 70.83024435698307\n",
      "gradient norm: 14.573844196362527\n",
      "epoch 235\n",
      "loss: 70.65483874728376\n",
      "gradient norm: 12.245715526048597\n",
      "epoch 236\n",
      "loss: 70.29204980156632\n",
      "gradient norm: 17.289326417988953\n",
      "epoch 237\n",
      "loss: 69.95923501375478\n",
      "gradient norm: 12.275882154046162\n",
      "epoch 238\n",
      "loss: 69.88116623133534\n",
      "gradient norm: 13.690845345753914\n",
      "epoch 239\n",
      "loss: 69.44442340888598\n",
      "gradient norm: 14.182021914242183\n",
      "epoch 240\n",
      "loss: 69.25309751412289\n",
      "gradient norm: 11.693140681570181\n",
      "epoch 241\n",
      "loss: 69.01022776983214\n",
      "gradient norm: 11.65815264611987\n",
      "epoch 242\n",
      "loss: 68.8213699924503\n",
      "gradient norm: 11.674033297746437\n",
      "epoch 243\n",
      "loss: 68.5625935282394\n",
      "gradient norm: 11.57892263987653\n",
      "epoch 244\n",
      "loss: 68.30668022715015\n",
      "gradient norm: 10.67883648440908\n",
      "epoch 245\n",
      "loss: 68.03740670933485\n",
      "gradient norm: 21.04624054274183\n",
      "epoch 246\n",
      "loss: 67.80811618312967\n",
      "gradient norm: 18.258634542407542\n",
      "epoch 247\n",
      "loss: 67.51333186649049\n",
      "gradient norm: 10.23619632196414\n",
      "epoch 248\n",
      "loss: 67.26412954266563\n",
      "gradient norm: 9.507935209435406\n",
      "epoch 249\n",
      "loss: 66.95022393827581\n",
      "gradient norm: 14.489844448203787\n",
      "epoch 250\n",
      "loss: 66.72903514022659\n",
      "gradient norm: 17.09678836857695\n",
      "epoch 251\n",
      "loss: 66.40931545524366\n",
      "gradient norm: 15.77748965399681\n",
      "epoch 252\n",
      "loss: 66.15013902342562\n",
      "gradient norm: 18.379606084569556\n",
      "epoch 253\n",
      "loss: 65.77292819319511\n",
      "gradient norm: 9.555804847645055\n",
      "epoch 254\n",
      "loss: 65.56410322268312\n",
      "gradient norm: 17.063970221368674\n",
      "epoch 255\n",
      "loss: 65.47917470908\n",
      "gradient norm: 18.52479507184261\n",
      "epoch 256\n",
      "loss: 65.18242629526641\n",
      "gradient norm: 23.24403036970783\n",
      "epoch 257\n",
      "loss: 65.03474302168291\n",
      "gradient norm: 21.057884799593317\n",
      "epoch 258\n",
      "loss: 64.66705137370798\n",
      "gradient norm: 11.906901603988196\n",
      "epoch 259\n",
      "loss: 64.45339415110021\n",
      "gradient norm: 8.741456456779966\n",
      "epoch 260\n",
      "loss: 64.30082758984906\n",
      "gradient norm: 10.817280185395733\n",
      "epoch 261\n",
      "loss: 64.06609292961281\n",
      "gradient norm: 11.86314758298736\n",
      "epoch 262\n",
      "loss: 63.70790236461288\n",
      "gradient norm: 10.30995107366128\n",
      "epoch 263\n",
      "loss: 63.3822983381293\n",
      "gradient norm: 12.535422062727324\n",
      "epoch 264\n",
      "loss: 63.137462880494965\n",
      "gradient norm: 12.082179926503409\n",
      "epoch 265\n",
      "loss: 62.74560559356554\n",
      "gradient norm: 12.463262727894799\n",
      "epoch 266\n",
      "loss: 62.49494783853681\n",
      "gradient norm: 13.119781898543728\n",
      "epoch 267\n",
      "loss: 62.261760950912546\n",
      "gradient norm: 14.577927729856482\n",
      "epoch 268\n",
      "loss: 61.99355073112297\n",
      "gradient norm: 14.429771192744509\n",
      "epoch 269\n",
      "loss: 61.601700525304366\n",
      "gradient norm: 10.955737412697239\n",
      "epoch 270\n",
      "loss: 61.329860252806235\n",
      "gradient norm: 11.016794797318557\n",
      "epoch 271\n",
      "loss: 61.18134566454139\n",
      "gradient norm: 13.306880410266302\n",
      "epoch 272\n",
      "loss: 60.87032510730846\n",
      "gradient norm: 12.814157994693181\n",
      "epoch 273\n",
      "loss: 60.65150068549995\n",
      "gradient norm: 19.389509380633626\n",
      "epoch 274\n",
      "loss: 60.40716076184955\n",
      "gradient norm: 16.000702641914494\n",
      "epoch 275\n",
      "loss: 60.079861758461696\n",
      "gradient norm: 13.688375434381141\n",
      "epoch 276\n",
      "loss: 59.85340982119005\n",
      "gradient norm: 14.613692983004816\n",
      "epoch 277\n",
      "loss: 59.661966587579826\n",
      "gradient norm: 12.590636462854079\n",
      "epoch 278\n",
      "loss: 59.21471855662617\n",
      "gradient norm: 14.679397497151848\n",
      "epoch 279\n",
      "loss: 58.993603665673\n",
      "gradient norm: 11.642242512331405\n",
      "epoch 280\n",
      "loss: 58.74045755451988\n",
      "gradient norm: 10.981379412243777\n",
      "epoch 281\n",
      "loss: 58.582823825516336\n",
      "gradient norm: 11.232827702461416\n",
      "epoch 282\n",
      "loss: 58.34427389588725\n",
      "gradient norm: 11.900366010071814\n",
      "epoch 283\n",
      "loss: 58.02178507951385\n",
      "gradient norm: 13.266956714041443\n",
      "epoch 284\n",
      "loss: 57.73807360956451\n",
      "gradient norm: 14.11268567423232\n",
      "epoch 285\n",
      "loss: 57.373366458825885\n",
      "gradient norm: 12.451245273902188\n",
      "epoch 286\n",
      "loss: 57.02118017477806\n",
      "gradient norm: 11.383004849120736\n",
      "epoch 287\n",
      "loss: 56.888922001376606\n",
      "gradient norm: 18.396667754644668\n",
      "epoch 288\n",
      "loss: 56.78582324217914\n",
      "gradient norm: 13.538619475186787\n",
      "epoch 289\n",
      "loss: 56.46134239710585\n",
      "gradient norm: 14.389751549703764\n",
      "epoch 290\n",
      "loss: 56.24251794982757\n",
      "gradient norm: 14.304707649962568\n",
      "epoch 291\n",
      "loss: 55.96396070162658\n",
      "gradient norm: 9.493034751448743\n",
      "epoch 292\n",
      "loss: 55.694783779505734\n",
      "gradient norm: 16.346999873095523\n",
      "epoch 293\n",
      "loss: 55.4428420666247\n",
      "gradient norm: 12.2083900864489\n",
      "epoch 294\n",
      "loss: 55.15624997104186\n",
      "gradient norm: 10.03286992595874\n",
      "epoch 295\n",
      "loss: 54.82043675510781\n",
      "gradient norm: 10.257000481188308\n",
      "epoch 296\n",
      "loss: 54.72640899487178\n",
      "gradient norm: 10.652588115299528\n",
      "epoch 297\n",
      "loss: 54.39784178922345\n",
      "gradient norm: 11.443582386609583\n",
      "epoch 298\n",
      "loss: 54.01281572508452\n",
      "gradient norm: 9.515857255529104\n",
      "epoch 299\n",
      "loss: 53.693445191588985\n",
      "gradient norm: 11.757922651894567\n",
      "epoch 300\n",
      "loss: 53.51701537588861\n",
      "gradient norm: 10.96406767220768\n",
      "epoch 301\n",
      "loss: 53.28786751998572\n",
      "gradient norm: 21.100643996222107\n",
      "epoch 302\n",
      "loss: 53.10362151945119\n",
      "gradient norm: 18.92309695060879\n",
      "epoch 303\n",
      "loss: 52.81655547561079\n",
      "gradient norm: 8.08775560599049\n",
      "epoch 304\n",
      "loss: 52.53025235753777\n",
      "gradient norm: 11.637466884023004\n",
      "epoch 305\n",
      "loss: 52.2956661640855\n",
      "gradient norm: 8.780172942318746\n",
      "epoch 306\n",
      "loss: 51.95430607960952\n",
      "gradient norm: 15.472254888644215\n",
      "epoch 307\n",
      "loss: 51.90072551133329\n",
      "gradient norm: 16.73924567027992\n",
      "epoch 308\n",
      "loss: 51.40484418881984\n",
      "gradient norm: 13.27250633842185\n",
      "epoch 309\n",
      "loss: 51.20568290007299\n",
      "gradient norm: 11.896994429306949\n",
      "epoch 310\n",
      "loss: 50.9481625365616\n",
      "gradient norm: 12.510085823076478\n",
      "epoch 311\n",
      "loss: 50.696960199159726\n",
      "gradient norm: 18.897618404982808\n",
      "epoch 312\n",
      "loss: 50.50785516042421\n",
      "gradient norm: 16.165325551485985\n",
      "epoch 313\n",
      "loss: 50.13105079593546\n",
      "gradient norm: 12.84502462331582\n",
      "epoch 314\n",
      "loss: 49.95514032944355\n",
      "gradient norm: 13.70752656753796\n",
      "epoch 315\n",
      "loss: 49.58207903758486\n",
      "gradient norm: 11.9751187824424\n",
      "epoch 316\n",
      "loss: 49.37608024714916\n",
      "gradient norm: 12.057732679300582\n",
      "epoch 317\n",
      "loss: 48.98637616580511\n",
      "gradient norm: 11.728255339936236\n",
      "epoch 318\n",
      "loss: 48.55242995978261\n",
      "gradient norm: 18.714653144523183\n",
      "epoch 319\n",
      "loss: 48.40434219985694\n",
      "gradient norm: 14.705086346664878\n",
      "epoch 320\n",
      "loss: 48.09818312225145\n",
      "gradient norm: 15.062698587937168\n",
      "epoch 321\n",
      "loss: 47.820982009273834\n",
      "gradient norm: 10.54408309710082\n",
      "epoch 322\n",
      "loss: 47.61038001721036\n",
      "gradient norm: 13.546673012717594\n",
      "epoch 323\n",
      "loss: 47.28524994106838\n",
      "gradient norm: 15.61519293559266\n",
      "epoch 324\n",
      "loss: 47.120400114879374\n",
      "gradient norm: 14.050990167146582\n",
      "epoch 325\n",
      "loss: 46.89329969981783\n",
      "gradient norm: 13.432033254778437\n",
      "epoch 326\n",
      "loss: 46.69941773290096\n",
      "gradient norm: 13.000179250635059\n",
      "epoch 327\n",
      "loss: 46.45620547944083\n",
      "gradient norm: 12.145517939458472\n",
      "epoch 328\n",
      "loss: 46.170340157095325\n",
      "gradient norm: 13.951556991931964\n",
      "epoch 329\n",
      "loss: 46.00804398767076\n",
      "gradient norm: 13.87006030161867\n",
      "epoch 330\n",
      "loss: 45.85661429816731\n",
      "gradient norm: 18.37520862673479\n",
      "epoch 331\n",
      "loss: 45.48173129391532\n",
      "gradient norm: 12.685993550801358\n",
      "epoch 332\n",
      "loss: 45.20800383405438\n",
      "gradient norm: 21.368155163285532\n",
      "epoch 333\n",
      "loss: 44.88850681478887\n",
      "gradient norm: 11.092630232416502\n",
      "epoch 334\n",
      "loss: 44.74285208352319\n",
      "gradient norm: 9.450711027787493\n",
      "epoch 335\n",
      "loss: 44.52342295128932\n",
      "gradient norm: 9.553975558028412\n",
      "epoch 336\n",
      "loss: 44.259694351787225\n",
      "gradient norm: 10.457569762434991\n",
      "epoch 337\n",
      "loss: 43.90381861697522\n",
      "gradient norm: 13.432395615545179\n",
      "epoch 338\n",
      "loss: 43.54985986757631\n",
      "gradient norm: 10.324275729568095\n",
      "epoch 339\n",
      "loss: 43.28165858965824\n",
      "gradient norm: 9.700313300838792\n",
      "epoch 340\n",
      "loss: 42.982895297238905\n",
      "gradient norm: 13.193120466278218\n",
      "epoch 341\n",
      "loss: 42.79389810328339\n",
      "gradient norm: 18.83581026131786\n",
      "epoch 342\n",
      "loss: 42.5469918254112\n",
      "gradient norm: 17.58553739931438\n",
      "epoch 343\n",
      "loss: 42.15874384616428\n",
      "gradient norm: 16.53704401557977\n",
      "epoch 344\n",
      "loss: 41.9189723401555\n",
      "gradient norm: 17.841600280568617\n",
      "epoch 345\n",
      "loss: 41.588579963144895\n",
      "gradient norm: 7.644113060114464\n",
      "epoch 346\n",
      "loss: 41.25603040186153\n",
      "gradient norm: 10.71886472289359\n",
      "epoch 347\n",
      "loss: 41.14431404361453\n",
      "gradient norm: 11.532375528796885\n",
      "epoch 348\n",
      "loss: 40.783936810677666\n",
      "gradient norm: 13.318676345105526\n",
      "epoch 349\n",
      "loss: 40.489268270262684\n",
      "gradient norm: 21.222395065631915\n",
      "epoch 350\n",
      "loss: 40.01467741274608\n",
      "gradient norm: 7.48559210013623\n",
      "epoch 351\n",
      "loss: 39.723946687572685\n",
      "gradient norm: 6.7429617982048455\n",
      "epoch 352\n",
      "loss: 39.56474214571393\n",
      "gradient norm: 13.330798433221048\n",
      "epoch 353\n",
      "loss: 39.30774911681878\n",
      "gradient norm: 17.137969334254343\n",
      "epoch 354\n",
      "loss: 38.97190754965924\n",
      "gradient norm: 14.84209874960229\n",
      "epoch 355\n",
      "loss: 38.6279145987595\n",
      "gradient norm: 15.621250904995126\n",
      "epoch 356\n",
      "loss: 38.34748797606373\n",
      "gradient norm: 14.609934294326166\n",
      "epoch 357\n",
      "loss: 38.19511972668393\n",
      "gradient norm: 17.55721312532171\n",
      "epoch 358\n",
      "loss: 37.849440673926075\n",
      "gradient norm: 19.808495414093954\n",
      "epoch 359\n",
      "loss: 37.58104073774988\n",
      "gradient norm: 19.024085067273784\n",
      "epoch 360\n",
      "loss: 37.47006399380805\n",
      "gradient norm: 17.12494230948641\n",
      "epoch 361\n",
      "loss: 37.069175113902446\n",
      "gradient norm: 12.764314779144328\n",
      "epoch 362\n",
      "loss: 36.842329686834205\n",
      "gradient norm: 11.368893853172015\n",
      "epoch 363\n",
      "loss: 36.573314471739415\n",
      "gradient norm: 9.662122655021607\n",
      "epoch 364\n",
      "loss: 36.17678532908891\n",
      "gradient norm: 10.35133887656067\n",
      "epoch 365\n",
      "loss: 36.0250755611968\n",
      "gradient norm: 10.339943816892076\n",
      "epoch 366\n",
      "loss: 35.647165467467076\n",
      "gradient norm: 17.63661544740061\n",
      "epoch 367\n",
      "loss: 35.23882228394878\n",
      "gradient norm: 9.368120214012821\n",
      "epoch 368\n",
      "loss: 35.01429416028117\n",
      "gradient norm: 6.381877035187648\n",
      "epoch 369\n",
      "loss: 34.692152595505874\n",
      "gradient norm: 9.034429736622362\n",
      "epoch 370\n",
      "loss: 34.531741186688635\n",
      "gradient norm: 10.159707698934596\n",
      "epoch 371\n",
      "loss: 34.13308142007069\n",
      "gradient norm: 10.96263681168067\n",
      "epoch 372\n",
      "loss: 33.765348854357526\n",
      "gradient norm: 13.912237248356007\n",
      "epoch 373\n",
      "loss: 33.58717271014864\n",
      "gradient norm: 11.107949452272942\n",
      "epoch 374\n",
      "loss: 33.306306119117394\n",
      "gradient norm: 13.413380388300833\n",
      "epoch 375\n",
      "loss: 32.99830661837531\n",
      "gradient norm: 12.109678923171169\n",
      "epoch 376\n",
      "loss: 32.73881575362894\n",
      "gradient norm: 11.165610278353613\n",
      "epoch 377\n",
      "loss: 32.353857804550785\n",
      "gradient norm: 17.771141624362638\n",
      "epoch 378\n",
      "loss: 32.10652214180904\n",
      "gradient norm: 13.221268701247917\n",
      "epoch 379\n",
      "loss: 31.817355728285875\n",
      "gradient norm: 14.05520043970885\n",
      "epoch 380\n",
      "loss: 31.592391674823016\n",
      "gradient norm: 12.694422892320208\n",
      "epoch 381\n",
      "loss: 31.24832740211201\n",
      "gradient norm: 17.209047284054535\n",
      "epoch 382\n",
      "loss: 30.927623179223325\n",
      "gradient norm: 10.234012176662443\n",
      "epoch 383\n",
      "loss: 30.71141965336592\n",
      "gradient norm: 7.769845625045816\n",
      "epoch 384\n",
      "loss: 30.355468871575958\n",
      "gradient norm: 13.158064226053607\n",
      "epoch 385\n",
      "loss: 30.089475135845323\n",
      "gradient norm: 14.323281620339738\n",
      "epoch 386\n",
      "loss: 29.76298299009863\n",
      "gradient norm: 14.502141995003612\n",
      "epoch 387\n",
      "loss: 29.392748052864597\n",
      "gradient norm: 12.335938015230758\n",
      "epoch 388\n",
      "loss: 29.066515064667424\n",
      "gradient norm: 17.730782705126273\n",
      "epoch 389\n",
      "loss: 28.90475270528258\n",
      "gradient norm: 15.163162348316014\n",
      "epoch 390\n",
      "loss: 28.62516843127677\n",
      "gradient norm: 14.543814832398644\n",
      "epoch 391\n",
      "loss: 28.208910920479052\n",
      "gradient norm: 10.745494249316012\n",
      "epoch 392\n",
      "loss: 28.00384447539412\n",
      "gradient norm: 11.36092740883676\n",
      "epoch 393\n",
      "loss: 27.672929806959477\n",
      "gradient norm: 12.633643659042956\n",
      "epoch 394\n",
      "loss: 27.345095736497324\n",
      "gradient norm: 11.339704255982367\n",
      "epoch 395\n",
      "loss: 27.1217764574424\n",
      "gradient norm: 11.544876552379593\n",
      "epoch 396\n",
      "loss: 26.879425262210574\n",
      "gradient norm: 23.548015811767364\n",
      "epoch 397\n",
      "loss: 26.461806748860894\n",
      "gradient norm: 14.575630714071698\n",
      "epoch 398\n",
      "loss: 26.246994662871753\n",
      "gradient norm: 9.364127153843974\n",
      "epoch 399\n",
      "loss: 25.929440410520336\n",
      "gradient norm: 8.619833658637255\n",
      "epoch 400\n",
      "loss: 25.710793940506097\n",
      "gradient norm: 17.424928644545773\n",
      "epoch 401\n",
      "loss: 25.54603567462159\n",
      "gradient norm: 18.972448674234226\n",
      "epoch 402\n",
      "loss: 25.28070820905621\n",
      "gradient norm: 21.008408638703873\n",
      "epoch 403\n",
      "loss: 24.81152599155231\n",
      "gradient norm: 10.708893297480454\n",
      "epoch 404\n",
      "loss: 24.557997061214557\n",
      "gradient norm: 12.195582407343547\n",
      "epoch 405\n",
      "loss: 24.38606945211274\n",
      "gradient norm: 17.649348301565155\n",
      "epoch 406\n",
      "loss: 23.94712106053541\n",
      "gradient norm: 17.71850847558146\n",
      "epoch 407\n",
      "loss: 23.68456010012722\n",
      "gradient norm: 13.77139231612347\n",
      "epoch 408\n",
      "loss: 23.36493326223442\n",
      "gradient norm: 18.769520982001183\n",
      "epoch 409\n",
      "loss: 23.08776112781007\n",
      "gradient norm: 14.347548428033356\n",
      "epoch 410\n",
      "loss: 22.85071545483498\n",
      "gradient norm: 18.546884718978866\n",
      "epoch 411\n",
      "loss: 22.63765212783842\n",
      "gradient norm: 18.618497339761433\n",
      "epoch 412\n",
      "loss: 22.359201570609173\n",
      "gradient norm: 12.453941786621908\n",
      "epoch 413\n",
      "loss: 22.06596425672121\n",
      "gradient norm: 13.201199840843495\n",
      "epoch 414\n",
      "loss: 21.86253092780085\n",
      "gradient norm: 15.025922252047472\n",
      "epoch 415\n",
      "loss: 21.52519562284765\n",
      "gradient norm: 16.45869734538819\n",
      "epoch 416\n",
      "loss: 21.167049138662364\n",
      "gradient norm: 11.072094056161262\n",
      "epoch 417\n",
      "loss: 20.932316957996218\n",
      "gradient norm: 14.076144351305626\n",
      "epoch 418\n",
      "loss: 20.727008440575844\n",
      "gradient norm: 17.362076944221233\n",
      "epoch 419\n",
      "loss: 20.457290266286623\n",
      "gradient norm: 16.817769531908414\n",
      "epoch 420\n",
      "loss: 20.1276072630613\n",
      "gradient norm: 16.860292345587805\n",
      "epoch 421\n",
      "loss: 19.832828157523238\n",
      "gradient norm: 11.631242619027727\n",
      "epoch 422\n",
      "loss: 19.675413281935597\n",
      "gradient norm: 17.89728326006312\n",
      "epoch 423\n",
      "loss: 19.260215556385102\n",
      "gradient norm: 11.781503077020908\n",
      "epoch 424\n",
      "loss: 18.948108147457035\n",
      "gradient norm: 13.351498002570066\n",
      "epoch 425\n",
      "loss: 18.613683137847673\n",
      "gradient norm: 11.264235370442272\n",
      "epoch 426\n",
      "loss: 18.26819802626274\n",
      "gradient norm: 9.481858123742402\n",
      "epoch 427\n",
      "loss: 18.066781571520277\n",
      "gradient norm: 9.964092152355967\n",
      "epoch 428\n",
      "loss: 17.869026829159775\n",
      "gradient norm: 20.38964886664821\n",
      "epoch 429\n",
      "loss: 17.576239095650376\n",
      "gradient norm: 16.100853766230372\n",
      "epoch 430\n",
      "loss: 17.396868024099362\n",
      "gradient norm: 25.54067128604293\n",
      "epoch 431\n",
      "loss: 16.992912902823214\n",
      "gradient norm: 17.66408280995253\n",
      "epoch 432\n",
      "loss: 16.820439465332854\n",
      "gradient norm: 11.085498654777934\n",
      "epoch 433\n",
      "loss: 16.637488762194362\n",
      "gradient norm: 14.490107664926592\n",
      "epoch 434\n",
      "loss: 16.32830287091021\n",
      "gradient norm: 16.179364398305253\n",
      "epoch 435\n",
      "loss: 16.0040041903261\n",
      "gradient norm: 8.391108858643072\n",
      "epoch 436\n",
      "loss: 15.698235764829288\n",
      "gradient norm: 10.801054060470898\n",
      "epoch 437\n",
      "loss: 15.496573641918495\n",
      "gradient norm: 10.230762627340036\n",
      "epoch 438\n",
      "loss: 15.117851293538708\n",
      "gradient norm: 16.003094700373445\n",
      "epoch 439\n",
      "loss: 14.92791569872206\n",
      "gradient norm: 16.315713218032673\n",
      "epoch 440\n",
      "loss: 14.553538969803139\n",
      "gradient norm: 17.10580115946754\n",
      "epoch 441\n",
      "loss: 14.236842625940168\n",
      "gradient norm: 18.139360369326152\n",
      "epoch 442\n",
      "loss: 13.86262105080392\n",
      "gradient norm: 14.106605375730913\n",
      "epoch 443\n",
      "loss: 13.521889723808973\n",
      "gradient norm: 13.015260365306647\n",
      "epoch 444\n",
      "loss: 13.363089973821046\n",
      "gradient norm: 12.95053454064413\n",
      "epoch 445\n",
      "loss: 13.008757700475453\n",
      "gradient norm: 15.344582351959009\n",
      "epoch 446\n",
      "loss: 12.820879275173338\n",
      "gradient norm: 21.556921659535213\n",
      "epoch 447\n",
      "loss: 12.538411563211515\n",
      "gradient norm: 19.8080122467502\n",
      "epoch 448\n",
      "loss: 12.220113213034827\n",
      "gradient norm: 12.441478574120728\n",
      "epoch 449\n",
      "loss: 11.878669603692014\n",
      "gradient norm: 11.771659583223116\n",
      "epoch 450\n",
      "loss: 11.569755345535627\n",
      "gradient norm: 12.085322345623403\n",
      "epoch 451\n",
      "loss: 11.161427253039463\n",
      "gradient norm: 12.878130725525345\n",
      "epoch 452\n",
      "loss: 10.96645344351192\n",
      "gradient norm: 13.608960969100938\n",
      "epoch 453\n",
      "loss: 10.813052902039312\n",
      "gradient norm: 12.459042145773116\n",
      "epoch 454\n",
      "loss: 10.443086933227761\n",
      "gradient norm: 9.854801416082717\n",
      "epoch 455\n",
      "loss: 10.140295121559438\n",
      "gradient norm: 13.465158220543927\n",
      "epoch 456\n",
      "loss: 9.914732369395297\n",
      "gradient norm: 13.375544353092636\n",
      "epoch 457\n",
      "loss: 9.606539909007774\n",
      "gradient norm: 23.110717983167564\n",
      "epoch 458\n",
      "loss: 9.156278326018814\n",
      "gradient norm: 17.00365678581879\n",
      "epoch 459\n",
      "loss: 8.933235890566472\n",
      "gradient norm: 14.848512259646595\n",
      "epoch 460\n",
      "loss: 8.625720734051969\n",
      "gradient norm: 17.96754334578227\n",
      "epoch 461\n",
      "loss: 8.46443992436897\n",
      "gradient norm: 15.792825993559928\n",
      "epoch 462\n",
      "loss: 8.01947960027565\n",
      "gradient norm: 12.208530499524276\n",
      "epoch 463\n",
      "loss: 7.899092116779596\n",
      "gradient norm: 15.778035612152708\n",
      "epoch 464\n",
      "loss: 7.531383943874523\n",
      "gradient norm: 17.502370392989892\n",
      "epoch 465\n",
      "loss: 7.108177032494137\n",
      "gradient norm: 9.712701515627934\n",
      "epoch 466\n",
      "loss: 6.9178592287325404\n",
      "gradient norm: 9.808786240842235\n",
      "epoch 467\n",
      "loss: 6.72533120169577\n",
      "gradient norm: 19.702125130695432\n",
      "epoch 468\n",
      "loss: 6.268679584669977\n",
      "gradient norm: 9.585572893238712\n",
      "epoch 469\n",
      "loss: 5.9682829830661435\n",
      "gradient norm: 8.385667548665296\n",
      "epoch 470\n",
      "loss: 5.765796214204029\n",
      "gradient norm: 13.04844402008282\n",
      "epoch 471\n",
      "loss: 5.488077435901337\n",
      "gradient norm: 14.42337164927613\n",
      "epoch 472\n",
      "loss: 5.145039218891412\n",
      "gradient norm: 17.24938048884499\n",
      "epoch 473\n",
      "loss: 4.873754459342523\n",
      "gradient norm: 15.527838209970305\n",
      "epoch 474\n",
      "loss: 4.8213870395673135\n",
      "gradient norm: 22.258424566211104\n",
      "epoch 475\n",
      "loss: 4.26472406387335\n",
      "gradient norm: 10.391047500797136\n",
      "epoch 476\n",
      "loss: 4.073648844198161\n",
      "gradient norm: 13.549258522350229\n",
      "epoch 477\n",
      "loss: 3.7906634622042352\n",
      "gradient norm: 9.905526803602527\n",
      "epoch 478\n",
      "loss: 3.601694000539174\n",
      "gradient norm: 10.313014873899144\n",
      "epoch 479\n",
      "loss: 3.274123906078744\n",
      "gradient norm: 10.033991685744525\n",
      "epoch 480\n",
      "loss: 2.9507076970727066\n",
      "gradient norm: 12.484975984316394\n",
      "epoch 481\n",
      "loss: 2.607935973261256\n",
      "gradient norm: 11.880430538036345\n",
      "epoch 482\n",
      "loss: 2.4298618208083185\n",
      "gradient norm: 10.873873530111913\n",
      "epoch 483\n",
      "loss: 2.0999609104084818\n",
      "gradient norm: 10.120185804807099\n",
      "epoch 484\n",
      "loss: 1.8636638254663442\n",
      "gradient norm: 13.349799762155758\n",
      "epoch 485\n",
      "loss: 1.5727587735134785\n",
      "gradient norm: 12.120493301366485\n",
      "epoch 486\n",
      "loss: 1.2693923187956564\n",
      "gradient norm: 8.564364089496078\n",
      "epoch 487\n",
      "loss: 1.0997168344800097\n",
      "gradient norm: 8.382036261418879\n",
      "epoch 488\n",
      "loss: 0.8212194890764919\n",
      "gradient norm: 7.912950656648112\n",
      "epoch 489\n",
      "loss: 0.6563709717859225\n",
      "gradient norm: 9.753471281327704\n",
      "epoch 490\n",
      "loss: 0.5284563379590312\n",
      "gradient norm: 11.181398178967132\n",
      "epoch 491\n",
      "loss: 0.35480323723836443\n",
      "gradient norm: 9.347491622532829\n",
      "epoch 492\n",
      "loss: 0.3005905845479231\n",
      "gradient norm: 6.788561594897274\n",
      "epoch 493\n",
      "loss: 0.22357918135476604\n",
      "gradient norm: 3.077372399255725\n",
      "epoch 494\n",
      "loss: 0.16199626274432838\n",
      "gradient norm: 5.497679057128353\n",
      "epoch 495\n",
      "loss: 0.15270072467358592\n",
      "gradient norm: 7.637791464847332\n",
      "epoch 496\n",
      "loss: 0.09510827395584695\n",
      "gradient norm: 6.923355345477572\n",
      "epoch 497\n",
      "loss: 0.06956723004256257\n",
      "gradient norm: 6.377610888177825\n",
      "epoch 498\n",
      "loss: 0.03677362757900196\n",
      "gradient norm: 4.289175466010231\n",
      "epoch 499\n",
      "loss: 0.03277231337731441\n",
      "gradient norm: 2.372088127248186\n",
      "epoch 500\n",
      "loss: 0.02540456948947934\n",
      "gradient norm: 2.0336681810694928\n",
      "epoch 501\n",
      "loss: 0.018784494520385064\n",
      "gradient norm: 2.5514747889789184\n",
      "epoch 502\n",
      "loss: 0.010496859065820164\n",
      "gradient norm: 2.5123071235826986\n",
      "epoch 503\n",
      "loss: 0.006148619539152322\n",
      "gradient norm: 3.039309607590692\n",
      "epoch 504\n",
      "loss: 0.0022871746075805097\n",
      "gradient norm: 1.5886634233064334\n",
      "epoch 505\n",
      "loss: 0.0004160584054863903\n",
      "gradient norm: 0.4828509694543828\n",
      "epoch 506\n",
      "loss: 0.00011633822717123016\n",
      "gradient norm: 0.30423468593257635\n",
      "epoch 507\n",
      "loss: 1.952760349337398e-05\n",
      "gradient norm: 0.14593325876505553\n",
      "epoch 508\n",
      "loss: 1.1267286477309149e-06\n",
      "gradient norm: 0.03756648354017055\n",
      "epoch 509\n",
      "loss: 1.1949794911912722e-07\n",
      "gradient norm: 0.012881275239577779\n",
      "epoch 510\n",
      "loss: 1.523928791872153e-08\n",
      "gradient norm: 0.004966148610809425\n",
      "epoch 511\n",
      "loss: 1.881606653817638e-09\n",
      "gradient norm: 0.0016931185729321438\n",
      "epoch 512\n",
      "loss: 7.964647020200904e-11\n",
      "gradient norm: 0.0003830629803362213\n",
      "epoch 513\n",
      "loss: 1.4784856313011753e-11\n",
      "gradient norm: 0.0001494114900171014\n",
      "epoch 514\n",
      "loss: 4.2236333813314677e-13\n",
      "gradient norm: 2.6774104001178886e-05\n",
      "epoch 515\n",
      "loss: 1.381121261718967e-13\n",
      "gradient norm: 1.4788146686044607e-05\n",
      "epoch 516\n",
      "loss: 2.7670336580670487e-15\n",
      "gradient norm: 2.1026179602949885e-06\n",
      "epoch 517\n",
      "loss: 8.993330405575616e-16\n",
      "gradient norm: 1.2981971679680656e-06\n",
      "epoch 518\n",
      "loss: 1.5454592173727736e-17\n",
      "gradient norm: 1.5081919377375513e-07\n",
      "epoch 519\n",
      "loss: 5.9782197814321214e-18\n",
      "gradient norm: 9.153013346997953e-08\n",
      "epoch 520\n",
      "loss: 9.183983238149857e-20\n",
      "gradient norm: 1.203871169811067e-08\n",
      "epoch 521\n",
      "loss: 2.5449728259863805e-20\n",
      "gradient norm: 6.890859582473256e-09\n",
      "\n",
      "\n",
      "Norm of the difference between real solution and found solution: 5.6046090573697144e-11\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
