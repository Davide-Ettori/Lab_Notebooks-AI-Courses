{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNWgVsFWCHZ4kicE3q6gQM4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZCDNUgw0nI3"
   },
   "source": [
    "# Newton method"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IJcJCwFhc8cs",
    "ExecuteTime": {
     "end_time": "2023-12-12T15:18:36.766638Z",
     "start_time": "2023-12-12T15:18:33.592080Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "# We enable double precision in JAX\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qkdJdGg1AP1"
   },
   "source": [
    "We consider a random matrix $A \\in \\mathbb{R}^{n\\times n}$, with $n = 100$ and a random vector $\\mathbf{x}_{\\text{ex}} \\in \\mathbb{R}^n$.\n",
    "We define then $\\mathbf{b} = A \\, \\mathbf{x}_{\\text{ex}}$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c0h8ihCddDPf",
    "ExecuteTime": {
     "end_time": "2023-12-12T15:34:27.015893Z",
     "start_time": "2023-12-12T15:34:27.006429Z"
    }
   },
   "source": [
    "n = 100\n",
    "\n",
    "np.random.seed(0)\n",
    "A = np.random.randn(n,n)\n",
    "x_ex = np.random.randn(n)\n",
    "b = A @ x_ex"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UanVhF4xAVoX"
   },
   "source": [
    "Define the loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    return jnp.sum(jnp.square(A @ x - b))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T15:34:27.921599Z",
     "start_time": "2023-12-12T15:34:27.914086Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAZ9XGaiAs3X"
   },
   "source": [
    "By using the `jax` library, implement and compile functins returning the gradient ($\\nabla \\mathcal{J}(\\mathbf{x})$) and the hessian ($\\nabla^2 \\mathcal{J}(\\mathbf{x})$) of the loss function (*Hint*: use the `jacrev` or the `jacfwd`) function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KflmuLXld2T4",
    "ExecuteTime": {
     "end_time": "2023-12-12T15:36:38.424264Z",
     "start_time": "2023-12-12T15:36:38.413693Z"
    }
   },
   "source": [
    "grad = jax.grad(loss, argnums=0)\n",
    "hess = jax.jacrev(jax.jacrev(loss))\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(grad)\n",
    "hess_jit = jax.jit(hess)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSMg8ocDBndO"
   },
   "source": [
    "Check that the results are correct (up to machine precision)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xZulGRQ1efFP",
    "ExecuteTime": {
     "end_time": "2023-12-12T15:37:33.058247Z",
     "start_time": "2023-12-12T15:37:32.800236Z"
    }
   },
   "source": [
    "np.random.seed(0)\n",
    "x_guess = np.random.randn(n)\n",
    "\n",
    "G_ad = grad_jit(x_guess)\n",
    "G_ex = 2 * A.T @ (A @ x_guess - b)\n",
    "print(np.linalg.norm(G_ad - G_ex))\n",
    "\n",
    "H_ad = hess_jit(x_guess)\n",
    "H_ex = 2 * A.T @ A\n",
    "print(np.linalg.norm(H_ad - H_ex))"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.191240070771156e-12\n",
      "5.490834437635708e-13\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-gA_kKPB2SV"
   },
   "source": [
    "Exploit the formula\n",
    "$$\n",
    "\\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v} = \\nabla_{\\mathbf{x}} \\phi(\\mathbf{x}, \\mathbf{v})\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\phi(\\mathbf{x}, \\mathbf{v}) := \\nabla \\mathcal{J}(\\mathbf{x}) \\cdot \\mathbf{v}\n",
    "$$\n",
    "to write an optimized function returning the hessian-vector-product\n",
    "$$\n",
    "(\\mathbf{x}, \\mathbf{v}) \\mapsto \\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v}.\n",
    "$$\n",
    "Compare the computational performance w.r.t. the full hessian computation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T9969dU4kc6f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1638461194311,
     "user_tz": -60,
     "elapsed": 10,
     "user": {
      "displayName": "Francesco Regazzoni",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08593163129562527691"
     }
    },
    "outputId": "368b2173-e971-474c-e3b1-f649d9bb15d1",
    "ExecuteTime": {
     "end_time": "2023-12-12T15:50:30.781820Z",
     "start_time": "2023-12-12T15:50:30.683517Z"
    }
   },
   "source": [
    "np.random.seed(1)\n",
    "v = np.random.randn(n)\n",
    "\n",
    "hvp_basic = lambda x, v: hess(x) @ v\n",
    "phi = lambda x, v: grad(x) @ v\n",
    "hvp = jax.grad(phi, argnums=0)\n",
    "\n",
    "hvp_basic_jit = jax.jit(hvp_basic)\n",
    "hvp_jit = jax.jit(hvp)\n",
    "\n",
    "Hv_ad = hvp_jit(x_guess, v)\n",
    "Hv_ex = H_ex @ v\n",
    "print(np.linalg.norm(Hv_ad - Hv_ex))"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.255129127373075e-12\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jsA4eUnuj3ju",
    "ExecuteTime": {
     "end_time": "2023-12-12T15:50:57.588603Z",
     "start_time": "2023-12-12T15:50:41.690568Z"
    }
   },
   "source": [
    "%timeit hvp_basic_jit(x_guess, v)\n",
    "%timeit hvp_jit(x_guess, v)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 µs ± 14.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "17.5 µs ± 106 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TagmrdjG4Ww4"
   },
   "source": [
    "Implement the Newton method for the minimization of the loss function $\\mathcal{L}$. Set a maximim number of 100 iterations and a tolerance on the increment norm of $\\epsilon = 10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping at epoch 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "5.811046573607706e-14"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(x):\n",
    "    return jnp.sum(jnp.power(A @ x - b, 2))\n",
    "\n",
    "grad = jax.grad(loss, argnums=0)\n",
    "hess = jax.jacrev(jax.jacrev(loss))\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(grad)\n",
    "hess_jit = jax.jit(hess)\n",
    "\n",
    "epochs = range(100)\n",
    "eps = 1e-7\n",
    "w = np.random.normal(0, 1e6, x_ex.shape)\n",
    "for epoch in epochs:\n",
    "    H = hess_jit(w)\n",
    "    g = grad_jit(w)\n",
    "    if np.linalg.norm(g) <= eps:\n",
    "        print(f\"Early Stopping at epoch {epoch}\")\n",
    "        break\n",
    "    w -= np.linalg.inv(H) @ g\n",
    "\n",
    "np.linalg.norm(x_ex - w) # convergenza velocissima --> tipica dei metodi del secondo ordine"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T16:06:54.785070Z",
     "start_time": "2023-12-12T16:06:54.687894Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNL7303C4oTL"
   },
   "source": [
    "Repeat the optimization loop for the loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_4^4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping at epoch 54\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.0002935209989837215"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(x):\n",
    "    return jnp.sum(jnp.power(A @ x - b, 4))\n",
    "\n",
    "grad = jax.grad(loss, argnums=0)\n",
    "hess = jax.jacrev(jax.jacrev(loss))\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(grad)\n",
    "hess_jit = jax.jit(hess)\n",
    "\n",
    "epochs = range(100)\n",
    "eps = 1e-7\n",
    "w = np.random.normal(0, 1e5, x_ex.shape)\n",
    "for epoch in epochs:\n",
    "    H = hess_jit(w)\n",
    "    g = grad_jit(w)\n",
    "    if np.linalg.norm(g) <= eps:\n",
    "        print(f\"Early Stopping at epoch {epoch}\")\n",
    "        break\n",
    "    w -= np.linalg.inv(H) @ g\n",
    "\n",
    "np.linalg.norm(x_ex - w)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T16:01:55.256425Z",
     "start_time": "2023-12-12T16:01:55.075834Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLinAlgError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEarly Stopping at epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m     w \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mH\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m@\u001B[39m g\n\u001B[1;32m     22\u001B[0m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(x_ex \u001B[38;5;241m-\u001B[39m w)\n",
      "File \u001B[0;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36minv\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/numpy/linalg/linalg.py:538\u001B[0m, in \u001B[0;36minv\u001B[0;34m(a)\u001B[0m\n\u001B[1;32m    536\u001B[0m signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD->D\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m isComplexType(t) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124md->d\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    537\u001B[0m extobj \u001B[38;5;241m=\u001B[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001B[0;32m--> 538\u001B[0m ainv \u001B[38;5;241m=\u001B[39m \u001B[43m_umath_linalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minv\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrap(ainv\u001B[38;5;241m.\u001B[39mastype(result_t, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/numpy/linalg/linalg.py:89\u001B[0m, in \u001B[0;36m_raise_linalgerror_singular\u001B[0;34m(err, flag)\u001B[0m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_raise_linalgerror_singular\u001B[39m(err, flag):\n\u001B[0;32m---> 89\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LinAlgError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSingular matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mLinAlgError\u001B[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "def loss(x):\n",
    "    return jnp.sum(jnp.absolute(A @ x - b)) # la norma 1 non va bene --> matrice hessiana non invertibile\n",
    "\n",
    "grad = jax.grad(loss, argnums=0)\n",
    "hess = jax.jacrev(jax.jacrev(loss))\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(grad)\n",
    "hess_jit = jax.jit(hess)\n",
    "\n",
    "epochs = range(100)\n",
    "eps = 1e-7\n",
    "w = np.random.normal(0, 1e5, x_ex.shape)\n",
    "for epoch in epochs:\n",
    "    H = hess_jit(w)\n",
    "    g = grad_jit(w)\n",
    "    if np.linalg.norm(g) <= eps:\n",
    "        print(f\"Early Stopping at epoch {epoch}\")\n",
    "        break\n",
    "    w -= np.linalg.inv(H) @ g\n",
    "\n",
    "np.linalg.norm(x_ex - w)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T16:10:37.923228Z",
     "start_time": "2023-12-12T16:10:37.799525Z"
    }
   }
  }
 ]
}
