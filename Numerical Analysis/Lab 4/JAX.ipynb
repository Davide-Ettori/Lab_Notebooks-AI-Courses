{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "mount_file_id": "1aDiop5c-V90dbJAUkJjgfpwZHa4vvdOa",
   "authorship_tag": "ABX9TyMO/dG6kzcvxKD6k1drTHdL"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7q1DhWpV-Dw"
   },
   "source": [
    "# Auto-diff with JAX\n",
    "\n",
    "https://github.com/google/jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Snjf4u7vf0sa"
   },
   "source": [
    "JAX is a Google research project, developed by the former developers of [Autograd](https://github.com/hips/autograd), bringing together the potentialities of Autograd and the linear algebra accelerator [XLA](https://www.tensorflow.org/xla). It is based on three pillars:\n",
    "- `grad`: Automatic Differentiation\n",
    "- `jit`: Just-in-time compilation\n",
    "- `vmap`: Automatic vectorization.\n",
    "\n",
    "## Automatic differentiation in JAX\n",
    "\n",
    "JAX augments numpy and Python code with function transformations which make it trivial to perform operations common in machine learning programs. JAX's augmented numpy lives at `jax.numpy`. With a few exceptions, you can think of `jax.numpy` as directly interchangeable with `numpy`. As a general rule, you should use `jax.numpy` whenever you plan to use any of JAX's transformations.\n",
    "\n",
    "The function `df = jax.grad(f, argnums = 0)` takes the callable object `f` and returns another callable object, `df`, evaluating the gradient of `f` w.r.t. the argument(s) of index(es) `argnums`. For more information, check out the [documentation](https://jax.readthedocs.io/en/latest/jax.html?highlight=grad#jax.grad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBVAFA6LiZhv"
   },
   "source": [
    "**Example**\n",
    "\n",
    "We consider the function:\n",
    "$$\n",
    "f(x) = x \\sin(x^2)\n",
    "$$\n",
    "\n",
    "and we compute $f'(x_0)$ for $x_0 = 0.13$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pJsIHEuC0BwB",
    "ExecuteTime": {
     "end_time": "2023-11-02T17:19:28.974588Z",
     "start_time": "2023-11-02T17:19:28.957169Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "func = lambda x : x * jnp.sin(x ** 2)\n",
    "x0 = 0.13\n",
    "dfunc_AD = jax.grad(func, argnums=0)\n",
    "df_AD = dfunc_AD(x0)\n",
    "\n",
    "# analytical derivative\n",
    "dfunc = lambda x : np.sin(x**2)+2 * x**2 * np.cos(x**2)\n",
    "df_ex = dfunc(x0)\n",
    "\n",
    "print('df (ex): %f' % df_ex)\n",
    "print('df (AD): %f' % df_AD)\n",
    "\n",
    "print('err (AD): %e' % (abs(df_AD - df_ex)/abs(df_ex))) # errore percentuale minuscolo"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df (ex): 0.050694\n",
      "df (AD): 0.050694\n",
      "err (AD): 7.348529e-08\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate the execution times of the functions `func` and `dfunc_AD`."
   ],
   "metadata": {
    "id": "Jt667yP0fy73"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.27 ms ± 296 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dfunc_AD(x0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T21:06:31.050722Z",
     "start_time": "2023-10-31T21:06:28.309376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.4 µs ± 19 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit func(x0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T21:06:43.599267Z",
     "start_time": "2023-10-31T21:06:31.050378Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9ziWf_lcjLn"
   },
   "source": [
    "### Speed it up with JIT\n",
    "\n",
    "Compile the functions `func` and `dfunc_AD` using the [just-in-time compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation) utility `jax.jit`. \n",
    "\n",
    "With `f_jit = jax.jit(f)` a callable `f` is compiled into `f_jit`.\n",
    "\n",
    "Then, check that the compiled functions return the same results as the original ones. Finally, evaluate the execution times and compare it with the previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "dfunc_AD_jit = jax.jit(dfunc_AD)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T21:06:43.604141Z",
     "start_time": "2023-10-31T21:06:43.601979Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.42 µs ± 19.8 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dfunc_AD_jit(x0) # ora è veloce come una normale function evaluation --> ecco lo standard migliore che abbiamo sul mercato"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T21:06:48.837703Z",
     "start_time": "2023-10-31T21:06:43.615378Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# per le reti neurali userai solo backwards mode --> molti input e pochi output --> questo è l'unico modo per scalare a reti neurali profonde (backpropagation)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T21:06:48.842692Z",
     "start_time": "2023-10-31T21:06:48.839275Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "(1.0506943464279175, 0.12999999523162842)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func(x, y):\n",
    "    return x * jnp.sin(x ** 2) + x * y\n",
    "\n",
    "x0 = 0.13\n",
    "y0 = 1.0\n",
    "grad_x = jax.grad(func, argnums=0)(x0, y0) # argnums mi dice rispetto a cosa derivare --> x0 e y0 devono essere float\n",
    "grad_y = jax.grad(func, argnums=1)(x0, y0)\n",
    "\n",
    "float(grad_x), float(grad_y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T21:06:48.874373Z",
     "start_time": "2023-10-31T21:06:48.845501Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# ecco come usare jax per calcolare le derivate parziali puntuali di una funzioni multivariabile"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T21:06:48.874617Z",
     "start_time": "2023-10-31T21:06:48.859347Z"
    }
   }
  }
 ]
}
