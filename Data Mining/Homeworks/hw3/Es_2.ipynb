{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# CS483 - Colab 2\n",
        "## Frequent Pattern Mining in Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "kcxD8L9ORHZx",
        "outputId": "4cfd172a-9685-4f03-a0cc-c90608c4a91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab 2 Mascot\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://cdn.dribbble.com/users/222579/screenshots/1654898/stubby-ben-rex-roll.gif\" width=\"150\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "print(\"Colab 2 Mascot\")\n",
        "Image(url='https://cdn.dribbble.com/users/222579/screenshots/1654898/stubby-ben-rex-roll.gif',width=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's set up Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-qHai2252mI",
        "outputId": "c9d18b45-b40f-4fdc-9ad1-19310aa5aa0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 39.6 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123622 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJ71AKe91eh"
      },
      "source": [
        "Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n",
        "\n",
        "**Make sure to follow the interactive instructions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "If you executed the cells above, you should be able to see the dataset we will need for this Colab under the \"Files\" tab on the left panel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "twk-K-jilWK7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr-8fK-1lmY0"
      },
      "source": [
        "Let's initialize the Spark context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UOwtm2l7lePt"
      },
      "outputs": [],
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from graph-small.txt\n",
        "data = sc.textFile(\"graph-small.txt\")\n",
        "\n",
        "# Parse the data to extract source and destination nodes and store them as (source, destination) pairs\n",
        "edges = data.map(lambda line: tuple(map(int, line.split()))).distinct()\n",
        "\n",
        "# Get the list of nodes by extracting all the unique source and destination nodes\n",
        "nodes = edges.flatMap(lambda edge: [edge[0], edge[1]]).distinct()\n",
        "\n",
        "# Get the number of nodes in the graph\n",
        "n = nodes.count()\n",
        "\n",
        "# Function to initialize the ranks vector, storing ranks in memory as a dictionary\n",
        "def initialize_ranks_in_memory(n):\n",
        "    return {node: 1/n for node in range(1, n+1)}\n",
        "\n",
        "# Build the adjacency list as an RDD, storing each node along with its outgoing edges\n",
        "adj_list_rdd = edges.groupByKey().mapValues(list)\n",
        "\n",
        "# Define a function to distribute the rank of each node to its neighbors using RDD operations\n",
        "def distribute_ranks_using_rdd(ranks_in_memory, adj_list_rdd, beta=0.8):\n",
        "    # Broadcast the in-memory rank vector to all worker nodes\n",
        "    rank_broadcast = sc.broadcast(ranks_in_memory)\n",
        "\n",
        "    # Use the adjacency list RDD to calculate rank contributions\n",
        "    rank_contributions_rdd = adj_list_rdd.flatMap(lambda node_neighbors:\n",
        "        [(neighbor, rank_broadcast.value[node_neighbors[0]] / len(node_neighbors[1]) * beta)\n",
        "         for neighbor in node_neighbors[1]]\n",
        "    )\n",
        "\n",
        "    # Sum the rank contributions for each node\n",
        "    rank_sums_rdd = rank_contributions_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    # Collect the new rank sums as a dictionary to update the in-memory ranks\n",
        "    new_ranks = rank_sums_rdd.collectAsMap()\n",
        "\n",
        "    return new_ranks\n",
        "\n",
        "# Initialize PageRank vector (r) with equal values for all nodes, stored in memory\n",
        "ranks = initialize_ranks_in_memory(n)\n",
        "\n",
        "# Set the teleportation factor (1-beta)\n",
        "beta = 0.8\n",
        "teleport_value = (1 - beta) / n\n",
        "\n",
        "# Iteratively compute PageRank over 40 iterations\n",
        "for i in range(40):\n",
        "    print(\"Iteration: \", i + 1)\n",
        "    # Distribute ranks using the adjacency list and keep results in memory\n",
        "    rank_contributions = distribute_ranks_using_rdd(ranks, adj_list_rdd, beta)\n",
        "\n",
        "    # Update the rank vector by adding the teleportation factor and update the in-memory ranks\n",
        "    ranks = {node: rank_contributions.get(node, 0) + teleport_value for node in range(1, n+1)}\n",
        "\n",
        "# Sort the ranks by value\n",
        "final_ranks_sorted = sorted(ranks.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get the top 5 and bottom 5 nodes by PageRank score\n",
        "top_5 = final_ranks_sorted[:5]\n",
        "bottom_5 = final_ranks_sorted[-5:]\n",
        "\n",
        "# Print the results\n",
        "print(\"Top 5 nodes by PageRank score:\")\n",
        "for node, score in top_5:\n",
        "    print(f\"Node {node}, Score: {score}\")\n",
        "\n",
        "print(\"Bottom 5 nodes by PageRank score:\")\n",
        "for node, score in bottom_5:\n",
        "    print(f\"Node {node}, Score: {score}\")"
      ],
      "metadata": {
        "id": "FnS0dU-OnfTh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f1f7a3-6952-4a72-bbd5-0cad294a4d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  1\n",
            "Iteration:  2\n",
            "Iteration:  3\n",
            "Iteration:  4\n",
            "Iteration:  5\n",
            "Iteration:  6\n",
            "Iteration:  7\n",
            "Iteration:  8\n",
            "Iteration:  9\n",
            "Iteration:  10\n",
            "Iteration:  11\n",
            "Iteration:  12\n",
            "Iteration:  13\n",
            "Iteration:  14\n",
            "Iteration:  15\n",
            "Iteration:  16\n",
            "Iteration:  17\n",
            "Iteration:  18\n",
            "Iteration:  19\n",
            "Iteration:  20\n",
            "Iteration:  21\n",
            "Iteration:  22\n",
            "Iteration:  23\n",
            "Iteration:  24\n",
            "Iteration:  25\n",
            "Iteration:  26\n",
            "Iteration:  27\n",
            "Iteration:  28\n",
            "Iteration:  29\n",
            "Iteration:  30\n",
            "Iteration:  31\n",
            "Iteration:  32\n",
            "Iteration:  33\n",
            "Iteration:  34\n",
            "Iteration:  35\n",
            "Iteration:  36\n",
            "Iteration:  37\n",
            "Iteration:  38\n",
            "Iteration:  39\n",
            "Iteration:  40\n",
            "Top 5 nodes by PageRank score:\n",
            "Node 53, Score: 0.03573120223267161\n",
            "Node 14, Score: 0.034170906972591376\n",
            "Node 40, Score: 0.03363008718974389\n",
            "Node 1, Score: 0.030005979479788617\n",
            "Node 27, Score: 0.029720144201405386\n",
            "Bottom 5 nodes by PageRank score:\n",
            "Node 89, Score: 0.003922466019802269\n",
            "Node 37, Score: 0.003808204291611451\n",
            "Node 81, Score: 0.0036953517493609916\n",
            "Node 59, Score: 0.0036698606601272845\n",
            "Node 85, Score: 0.003409694077402821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from graph-full.txt\n",
        "data = sc.textFile(\"graph-full.txt\")\n",
        "\n",
        "# Parse the data to extract source and destination nodes and store them as (source, destination) pairs\n",
        "edges = data.map(lambda line: tuple(map(int, line.split()))).distinct()\n",
        "\n",
        "# Get the list of nodes by extracting all the unique source and destination nodes\n",
        "nodes = edges.flatMap(lambda edge: [edge[0], edge[1]]).distinct()\n",
        "\n",
        "# Get the number of nodes in the graph\n",
        "n = nodes.count()\n",
        "\n",
        "# Function to initialize the ranks vector, storing ranks in memory as a dictionary\n",
        "def initialize_ranks_in_memory(n):\n",
        "    return {node: 1/n for node in range(1, n+1)}\n",
        "\n",
        "# Build the adjacency list as an RDD, storing each node along with its outgoing edges\n",
        "adj_list_rdd = edges.groupByKey().mapValues(list)\n",
        "\n",
        "# Define a function to distribute the rank of each node to its neighbors using RDD operations\n",
        "def distribute_ranks_using_rdd(ranks_in_memory, adj_list_rdd, beta=0.8):\n",
        "    # Broadcast the in-memory rank vector to all worker nodes\n",
        "    rank_broadcast = sc.broadcast(ranks_in_memory)\n",
        "\n",
        "    # Use the adjacency list RDD to calculate rank contributions\n",
        "    rank_contributions_rdd = adj_list_rdd.flatMap(lambda node_neighbors:\n",
        "        [(neighbor, rank_broadcast.value[node_neighbors[0]] / len(node_neighbors[1]) * beta)\n",
        "         for neighbor in node_neighbors[1]]\n",
        "    )\n",
        "\n",
        "    # Sum the rank contributions for each node\n",
        "    rank_sums_rdd = rank_contributions_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    # Collect the new rank sums as a dictionary to update the in-memory ranks\n",
        "    new_ranks = rank_sums_rdd.collectAsMap()\n",
        "\n",
        "    return new_ranks\n",
        "\n",
        "# Initialize PageRank vector (r) with equal values for all nodes, stored in memory\n",
        "ranks = initialize_ranks_in_memory(n)\n",
        "\n",
        "# Set the teleportation factor (1-beta)\n",
        "beta = 0.8\n",
        "teleport_value = (1 - beta) / n\n",
        "\n",
        "# Iteratively compute PageRank over 40 iterations\n",
        "for i in range(40):\n",
        "    print(\"Iteration: \", i + 1)\n",
        "    # Distribute ranks using the adjacency list and keep results in memory\n",
        "    rank_contributions = distribute_ranks_using_rdd(ranks, adj_list_rdd, beta)\n",
        "\n",
        "    # Update the rank vector by adding the teleportation factor and update the in-memory ranks\n",
        "    ranks = {node: rank_contributions.get(node, 0) + teleport_value for node in range(1, n+1)}\n",
        "\n",
        "# Sort the ranks by value\n",
        "final_ranks_sorted = sorted(ranks.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get the top 5 and bottom 5 nodes by PageRank score\n",
        "top_5 = final_ranks_sorted[:5]\n",
        "bottom_5 = final_ranks_sorted[-5:]\n",
        "\n",
        "# Print the results\n",
        "print(\"Top 5 nodes by PageRank score:\")\n",
        "for node, score in top_5:\n",
        "    print(f\"Node {node}, Score: {score}\")\n",
        "\n",
        "print(\"Bottom 5 nodes by PageRank score:\")\n",
        "for node, score in bottom_5:\n",
        "    print(f\"Node {node}, Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMi7ULuue-_8",
        "outputId": "21146c68-48d4-425b-9f64-5baf1a11b704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  1\n",
            "Iteration:  2\n",
            "Iteration:  3\n",
            "Iteration:  4\n",
            "Iteration:  5\n",
            "Iteration:  6\n",
            "Iteration:  7\n",
            "Iteration:  8\n",
            "Iteration:  9\n",
            "Iteration:  10\n",
            "Iteration:  11\n",
            "Iteration:  12\n",
            "Iteration:  13\n",
            "Iteration:  14\n",
            "Iteration:  15\n",
            "Iteration:  16\n",
            "Iteration:  17\n",
            "Iteration:  18\n",
            "Iteration:  19\n",
            "Iteration:  20\n",
            "Iteration:  21\n",
            "Iteration:  22\n",
            "Iteration:  23\n",
            "Iteration:  24\n",
            "Iteration:  25\n",
            "Iteration:  26\n",
            "Iteration:  27\n",
            "Iteration:  28\n",
            "Iteration:  29\n",
            "Iteration:  30\n",
            "Iteration:  31\n",
            "Iteration:  32\n",
            "Iteration:  33\n",
            "Iteration:  34\n",
            "Iteration:  35\n",
            "Iteration:  36\n",
            "Iteration:  37\n",
            "Iteration:  38\n",
            "Iteration:  39\n",
            "Iteration:  40\n",
            "Top 5 nodes by PageRank score:\n",
            "Node 263, Score: 0.002020291181518219\n",
            "Node 537, Score: 0.00194334157145315\n",
            "Node 965, Score: 0.0019254478071662627\n",
            "Node 243, Score: 0.001852634016241731\n",
            "Node 285, Score: 0.0018273721700645142\n",
            "Bottom 5 nodes by PageRank score:\n",
            "Node 408, Score: 0.00038779848719291705\n",
            "Node 424, Score: 0.00035481538649301454\n",
            "Node 62, Score: 0.00035314810510596274\n",
            "Node 93, Score: 0.0003513568937516577\n",
            "Node 558, Score: 0.0003286018525215297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from graph-small.txt\n",
        "data = sc.textFile(\"graph-small.txt\")\n",
        "\n",
        "# Parse the data to extract source and destination nodes and store them as (source, destination) pairs\n",
        "edges = data.map(lambda line: tuple(map(int, line.split()))).distinct()\n",
        "\n",
        "# Get the list of nodes by extracting all the unique source and destination nodes\n",
        "nodes = edges.flatMap(lambda edge: [edge[0], edge[1]]).distinct()\n",
        "\n",
        "# Get the number of nodes in the graph\n",
        "n = nodes.count()\n",
        "\n",
        "# Initialize hub scores with all 1s\n",
        "hubbiness = nodes.map(lambda node: (node, 1.0))\n",
        "\n",
        "# Build the adjacency list, storing each node along with its outgoing edges\n",
        "adj_list = edges.groupByKey().mapValues(list)\n",
        "\n",
        "# Create the link matrix L and its transpose LT\n",
        "# L[i -> j] is represented as (i, j) for edges, LT is the reverse (j, i)\n",
        "link_matrix = adj_list\n",
        "link_matrix_T = edges.map(lambda edge: (edge[1], edge[0])).groupByKey().mapValues(list)\n",
        "\n",
        "# Function to compute authority vector from hubbiness\n",
        "def compute_authority(hubbiness_vector, link_matrix_T):\n",
        "    # Multiply LT by hubbiness (i.e. for each destination, sum up the hubbiness of incoming nodes)\n",
        "    auth_contributions = link_matrix_T.join(hubbiness_vector).flatMap(lambda x: [(node, x[1][1]) for node in x[1][0]])\n",
        "    authority_vector = auth_contributions.reduceByKey(lambda a, b: a + b)\n",
        "    # Scale the authority vector so that the max value is 1\n",
        "    max_auth = authority_vector.map(lambda x: x[1]).treeReduce(lambda a, b: np.max([a, b]))\n",
        "    authority_vector = authority_vector.mapValues(lambda v: v / max_auth)\n",
        "    return authority_vector\n",
        "\n",
        "# Function to compute hubbiness vector from authority\n",
        "def compute_hubbiness(authority_vector, link_matrix):\n",
        "    # Multiply L by authority (i.e. for each source, sum up the authority of outgoing nodes)\n",
        "    hub_contributions = link_matrix.join(authority_vector).flatMap(lambda x: [(node, x[1][1]) for node in x[1][0]])\n",
        "    hubbiness_vector = hub_contributions.reduceByKey(lambda a, b: a + b)\n",
        "    # Scale the hubbiness vector so that the max value is 1\n",
        "    max_hub = hubbiness_vector.map(lambda x: x[1]).treeReduce(lambda a, b: np.max([a, b]))\n",
        "    hubbiness_vector = hubbiness_vector.mapValues(lambda v: v / max_hub)\n",
        "    return hubbiness_vector\n",
        "\n",
        "# Iterate to compute hubbiness and authority scores\n",
        "for i in range(40):\n",
        "    print(\"Iteration: \", i + 1)\n",
        "    # Compute the authority vector from the current hubbiness vector\n",
        "    authority = compute_authority(hubbiness, link_matrix_T)\n",
        "\n",
        "    # Compute the hubbiness vector from the current authority vector\n",
        "    hubbiness = compute_hubbiness(authority, link_matrix)\n",
        "\n",
        "# Collect the final hubbiness and authority vectors\n",
        "final_hubbiness = hubbiness.collect()\n",
        "final_authority = authority.collect()\n",
        "\n",
        "final_hubbiness, final_authority = final_authority, final_hubbiness\n",
        "\n",
        "# Sort hubbiness and authority by score\n",
        "final_hubbiness_sorted = sorted(final_hubbiness, key=lambda x: x[1], reverse=True)\n",
        "final_authority_sorted = sorted(final_authority, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get the top 5 and bottom 5 nodes by hubbiness and authority scores\n",
        "top_5_hubbiness = final_hubbiness_sorted[:5]\n",
        "bottom_5_hubbiness = final_hubbiness_sorted[-5:]\n",
        "top_5_authority = final_authority_sorted[:5]\n",
        "bottom_5_authority = final_authority_sorted[-5:]\n",
        "\n",
        "# Print the results\n",
        "print(\"Top 5 nodes by Hubbiness score:\")\n",
        "for node, score in top_5_hubbiness:\n",
        "    print(f\"Node {node}, Score: {score}\")\n",
        "\n",
        "print(\"Bottom 5 nodes by Hubbiness score:\")\n",
        "for node, score in bottom_5_hubbiness:\n",
        "    print(f\"Node {node}, Score: {score}\")\n",
        "\n",
        "print(\"Top 5 nodes by Authority score:\")\n",
        "for node, score in top_5_authority:\n",
        "    print(f\"Node {node}, Score: {score}\")\n",
        "\n",
        "print(\"Bottom 5 nodes by Authority score:\")\n",
        "for node, score in bottom_5_authority:\n",
        "    print(f\"Node {node}, Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPwfiXUIgwbB",
        "outputId": "b0d8289b-9b04-4b2d-dd5e-e2df81774362"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  1\n",
            "Iteration:  2\n",
            "Iteration:  3\n",
            "Iteration:  4\n",
            "Iteration:  5\n",
            "Iteration:  6\n",
            "Iteration:  7\n",
            "Iteration:  8\n",
            "Iteration:  9\n",
            "Iteration:  10\n",
            "Iteration:  11\n",
            "Iteration:  12\n",
            "Iteration:  13\n",
            "Iteration:  14\n",
            "Iteration:  15\n",
            "Iteration:  16\n",
            "Iteration:  17\n",
            "Iteration:  18\n",
            "Iteration:  19\n",
            "Iteration:  20\n",
            "Iteration:  21\n",
            "Iteration:  22\n",
            "Iteration:  23\n",
            "Iteration:  24\n",
            "Iteration:  25\n",
            "Iteration:  26\n",
            "Iteration:  27\n",
            "Iteration:  28\n",
            "Iteration:  29\n",
            "Iteration:  30\n",
            "Iteration:  31\n",
            "Iteration:  32\n",
            "Iteration:  33\n",
            "Iteration:  34\n",
            "Iteration:  35\n",
            "Iteration:  36\n",
            "Iteration:  37\n",
            "Iteration:  38\n",
            "Iteration:  39\n",
            "Iteration:  40\n",
            "Top 5 nodes by Hubbiness score:\n",
            "Node 59, Score: 1.0\n",
            "Node 39, Score: 0.9810799133868425\n",
            "Node 22, Score: 0.9741107079593092\n",
            "Node 11, Score: 0.9574282616181096\n",
            "Node 58, Score: 0.9574262000187687\n",
            "Bottom 5 nodes by Hubbiness score:\n",
            "Node 53, Score: 0.23548212611307842\n",
            "Node 95, Score: 0.2297612686427331\n",
            "Node 15, Score: 0.22106736398255405\n",
            "Node 35, Score: 0.21233808216249786\n",
            "Node 9, Score: 0.20936882949300997\n",
            "Top 5 nodes by Authority score:\n",
            "Node 66, Score: 1.0\n",
            "Node 40, Score: 0.98253375206419\n",
            "Node 27, Score: 0.9567022310293514\n",
            "Node 53, Score: 0.895179582844727\n",
            "Node 1, Score: 0.8215488570187269\n",
            "Bottom 5 nodes by Authority score:\n",
            "Node 50, Score: 0.06971236451749908\n",
            "Node 67, Score: 0.0676041033486925\n",
            "Node 24, Score: 0.06366924801269334\n",
            "Node 33, Score: 0.055604337523114784\n",
            "Node 54, Score: 0.0485967640487227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from graph-full.txt\n",
        "data = sc.textFile(\"graph-full.txt\")\n",
        "\n",
        "# Parse the data to extract source and destination nodes and store them as (source, destination) pairs\n",
        "edges = data.map(lambda line: tuple(map(int, line.split()))).distinct()\n",
        "\n",
        "# Get the list of nodes by extracting all the unique source and destination nodes\n",
        "nodes = edges.flatMap(lambda edge: [edge[0], edge[1]]).distinct()\n",
        "\n",
        "# Get the number of nodes in the graph\n",
        "n = nodes.count()\n",
        "\n",
        "# Initialize hub scores with all 1s\n",
        "hubbiness = nodes.map(lambda node: (node, 1.0))\n",
        "\n",
        "# Build the adjacency list, storing each node along with its outgoing edges\n",
        "adj_list = edges.groupByKey().mapValues(list)\n",
        "\n",
        "# Create the link matrix L and its transpose LT\n",
        "# L[i -> j] is represented as (i, j) for edges, LT is the reverse (j, i)\n",
        "link_matrix = adj_list\n",
        "link_matrix_T = edges.map(lambda edge: (edge[1], edge[0])).groupByKey().mapValues(list)\n",
        "\n",
        "# Function to compute authority vector from hubbiness\n",
        "def compute_authority(hubbiness_vector, link_matrix_T):\n",
        "    # Multiply LT by hubbiness (i.e. for each destination, sum up the hubbiness of incoming nodes)\n",
        "    auth_contributions = link_matrix_T.join(hubbiness_vector).flatMap(lambda x: [(node, x[1][1]) for node in x[1][0]])\n",
        "    authority_vector = auth_contributions.reduceByKey(lambda a, b: a + b)\n",
        "    # Scale the authority vector so that the max value is 1\n",
        "    max_auth = authority_vector.map(lambda x: x[1]).treeReduce(lambda a, b: np.max([a, b]))\n",
        "    authority_vector = authority_vector.mapValues(lambda v: v / max_auth)\n",
        "    return authority_vector\n",
        "\n",
        "# Function to compute hubbiness vector from authority\n",
        "def compute_hubbiness(authority_vector, link_matrix):\n",
        "    # Multiply L by authority (i.e. for each source, sum up the authority of outgoing nodes)\n",
        "    hub_contributions = link_matrix.join(authority_vector).flatMap(lambda x: [(node, x[1][1]) for node in x[1][0]])\n",
        "    hubbiness_vector = hub_contributions.reduceByKey(lambda a, b: a + b)\n",
        "    # Scale the hubbiness vector so that the max value is 1\n",
        "    max_hub = hubbiness_vector.map(lambda x: x[1]).treeReduce(lambda a, b: np.max([a, b]))\n",
        "    hubbiness_vector = hubbiness_vector.mapValues(lambda v: v / max_hub)\n",
        "    return hubbiness_vector\n",
        "\n",
        "# Iterate to compute hubbiness and authority scores\n",
        "for i in range(40):\n",
        "    print(\"Iteration: \", i + 1)\n",
        "    # Compute the authority vector from the current hubbiness vector\n",
        "    authority = compute_authority(hubbiness, link_matrix_T)\n",
        "\n",
        "    # Compute the hubbiness vector from the current authority vector\n",
        "    hubbiness = compute_hubbiness(authority, link_matrix)\n",
        "\n",
        "# Collect the final hubbiness and authority vectors\n",
        "final_hubbiness = hubbiness.collect()\n",
        "final_authority = authority.collect()\n",
        "\n",
        "final_hubbiness, final_authority = final_authority, final_hubbiness\n",
        "\n",
        "# Sort hubbiness and authority by score\n",
        "final_hubbiness_sorted = sorted(final_hubbiness, key=lambda x: x[1], reverse=True)\n",
        "final_authority_sorted = sorted(final_authority, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get the top 5 and bottom 5 nodes by hubbiness and authority scores\n",
        "top_5_hubbiness = final_hubbiness_sorted[:5]\n",
        "bottom_5_hubbiness = final_hubbiness_sorted[-5:]\n",
        "top_5_authority = final_authority_sorted[:5]\n",
        "bottom_5_authority = final_authority_sorted[-5:]\n",
        "\n",
        "# Print the results\n",
        "print(\"Top 5 nodes by Hubbiness score:\")\n",
        "for node, score in top_5_hubbiness:\n",
        "    print(f\"Node {node}, Score: {score}\")\n",
        "\n",
        "print(\"Bottom 5 nodes by Hubbiness score:\")\n",
        "for node, score in bottom_5_hubbiness:\n",
        "    print(f\"Node {node}, Score: {score}\")\n",
        "\n",
        "print(\"Top 5 nodes by Authority score:\")\n",
        "for node, score in top_5_authority:\n",
        "    print(f\"Node {node}, Score: {score}\")\n",
        "\n",
        "print(\"Bottom 5 nodes by Authority score:\")\n",
        "for node, score in bottom_5_authority:\n",
        "    print(f\"Node {node}, Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MihA_k0j9O48",
        "outputId": "71d7a1a1-8c25-444d-b99f-621e0a44b103"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  1\n",
            "Iteration:  2\n",
            "Iteration:  3\n",
            "Iteration:  4\n",
            "Iteration:  5\n",
            "Iteration:  6\n",
            "Iteration:  7\n",
            "Iteration:  8\n",
            "Iteration:  9\n",
            "Iteration:  10\n",
            "Iteration:  11\n",
            "Iteration:  12\n",
            "Iteration:  13\n",
            "Iteration:  14\n",
            "Iteration:  15\n",
            "Iteration:  16\n",
            "Iteration:  17\n",
            "Iteration:  18\n",
            "Iteration:  19\n",
            "Iteration:  20\n",
            "Iteration:  21\n",
            "Iteration:  22\n",
            "Iteration:  23\n",
            "Iteration:  24\n",
            "Iteration:  25\n",
            "Iteration:  26\n",
            "Iteration:  27\n",
            "Iteration:  28\n",
            "Iteration:  29\n",
            "Iteration:  30\n",
            "Iteration:  31\n",
            "Iteration:  32\n",
            "Iteration:  33\n",
            "Iteration:  34\n",
            "Iteration:  35\n",
            "Iteration:  36\n",
            "Iteration:  37\n",
            "Iteration:  38\n",
            "Iteration:  39\n",
            "Iteration:  40\n",
            "Top 5 nodes by Hubbiness score:\n",
            "Node 840, Score: 1.0\n",
            "Node 155, Score: 0.9499618624906543\n",
            "Node 234, Score: 0.8986645288972261\n",
            "Node 389, Score: 0.8634171101843793\n",
            "Node 472, Score: 0.8632841092495216\n",
            "Bottom 5 nodes by Hubbiness score:\n",
            "Node 889, Score: 0.07678413939216452\n",
            "Node 539, Score: 0.06602659373418493\n",
            "Node 141, Score: 0.06453117646225177\n",
            "Node 835, Score: 0.05779059354433016\n",
            "Node 23, Score: 0.042066854890936534\n",
            "Top 5 nodes by Authority score:\n",
            "Node 893, Score: 1.0\n",
            "Node 16, Score: 0.9635572849634397\n",
            "Node 799, Score: 0.9510158161074017\n",
            "Node 146, Score: 0.9246703586198441\n",
            "Node 473, Score: 0.899866197360405\n",
            "Bottom 5 nodes by Authority score:\n",
            "Node 910, Score: 0.08571673456144877\n",
            "Node 24, Score: 0.08171239406816942\n",
            "Node 462, Score: 0.07544228624641901\n",
            "Node 135, Score: 0.06653910487622793\n",
            "Node 19, Score: 0.05608316377607618\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}